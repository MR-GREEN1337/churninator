# --- Model & Data ---
# We are NOT loading the base model. We are starting from our Stage 1 checkpoint.
base_model: "forge/checkpoints/churninator-smollm-1.7b-grounded-v1/final" # <-- CRITICAL: Path to your Stage 1 model
data_file: "stage2_reasoning.jsonl"
output_dir: "checkpoints/churninator-smollm-1.7b-reasoning-v1" # Final model output

# --- Training Hyperparameters ---
# We often use a slightly lower learning rate for the second stage of tuning.
training_mode: "lora"
lora_r: 32
lora_alpha: 64
lora_dropout: 0.05

learning_rate: 5.0e-5 # Lower LR for fine-tuning an already-tuned model
num_train_epochs: 1
per_device_train_batch_size: 1 # Reasoning data has longer sequences
gradient_accumulation_steps: 16
gradient_checkpointing: true
optim: "paged_adamw_8bit"

# --- Precision & Logging ---
bf16: true
fp16: false
report_to: "wandb"
logging_steps: 10
