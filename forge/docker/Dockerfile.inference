# forge/docker/Dockerfile.inference (UV Edition)

# ==============================================================================
# The Churninator - High-Performance Inference Server (Powered by UV)
# ==============================================================================
# Start from the smaller CUDA 'runtime' image.
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

# --- Stage 1: Setup Environment and System Dependencies ---
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    python3.11 \
    python3-pip \
    curl \
    && rm -rf /var/lib/apt/lists/*
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1

# --- Stage 2: Install UV and Python Dependencies ---
WORKDIR /app

# Install uv.
ADD https://astral.sh/uv/install.sh /uv-installer.sh
RUN sh /uv-installer.sh && rm /uv-installer.sh
ENV PATH="/root/.local/bin:${PATH}"

# Copy dependency files.
COPY ./pyproject.toml ./uv.lock* ./

# Create a virtual environment and install dependencies.
RUN uv venv
RUN uv sync --locked --system-site-packages

# Copy and install our local package.
COPY ./setup.py .
RUN uv pip install -e .

# --- Stage 3: Copy Application and Model Code ---
COPY ./backend/inference /app/backend/inference
COPY ./forge/utils /app/forge/utils

# Create the directory for mounting models.
RUN mkdir -p /app/models

# --- Final Configuration ---
# Activate the virtual environment.
ENV VIRTUAL_ENV=/app/.venv
ENV PATH="/app/.venv/bin:$PATH"

# Expose the API port.
EXPOSE 8001

# Run the inference server using uv run. This is the modern, fast
# way to execute commands within the managed environment.
CMD ["uv", "run", "uvicorn", "backend.inference.server:app", "--host", "0.0.0.0", "--port", "8001"]
