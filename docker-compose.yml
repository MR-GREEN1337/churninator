services:
  # -------------------------------------------------------------------------- #
  #                                 BACKEND SERVICES                           #
  # -------------------------------------------------------------------------- #

  postgres_db:
    image: postgres:17-alpine
    container_name: churninator_postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 5s
      retries: 5

  redis_queue:
    image: redis:7-alpine
    container_name: churninator_redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  backend_api:
    build:
      context: ./backend
      dockerfile: Dockerfile.api # Assumes you've created this Dockerfile
    container_name: churninator_api
    depends_on:
      postgres_db:
        condition: service_healthy
      redis_queue:
        condition: service_healthy
    ports:
      - "8000:8000"
    volumes:
      # Mount the source code for hot-reloading during development
      - ./backend/src:/app/src
    env_file:
      - .env
    command: uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload

  inference_server:
    build:
      context: ./
      dockerfile: backend/inference/Dockerfile.inference # Assumes you've created this
    container_name: churninator_inference
    volumes:
      # Mount your fine-tuned models from the forge into the container
      - ./forge/checkpoints:/app/models
    environment:
      # Point to your BEST fine-tuned model by default
      - MODEL_ID=/app/models/churninator-smollm-1.7b-reasoning-v1/final
    ports:
      - "8001:8001"
    # This is the key for GPU access with Docker Compose
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # Use all available GPUs
              capabilities: [gpu]

  worker:
    build:
      context: ./backend/worker
      dockerfile: Dockerfile.worker # Assumes you've created this
    container_name: churninator_worker
    depends_on:
      - redis_queue
      - inference_server
    volumes:
      # Mount source code for easy development
      - ./backend/src:/app/src
      - ./forge/utils:/app/forge/utils # The worker needs the function_parser
    env_file:
      - .env
    # We don't run the entrypoint directly, as it's defined in the Dockerfile

  # -------------------------------------------------------------------------- #
  #                                 FRONTEND SERVICE                           #
  # -------------------------------------------------------------------------- #

  frontend:
    build:
      context: ./web # Assuming your frontend is in a 'web' directory
      dockerfile: Dockerfile
    container_name: churninator_frontend
    depends_on:
      - backend_api
    ports:
      - "3000:3000"
    volumes:
      # Hot-reloading for the frontend
      - ./web:/app
      - /app/node_modules # Use the node_modules from within the container
      - /app/.next
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000

# --- Named Volumes ---
# This ensures your Postgres data persists even if you stop and remove containers
volumes:
  postgres_data:
