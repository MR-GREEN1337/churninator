# ==============================================================================
# Docker Compose for PRODUCTION (NVIDIA GPU Server)
# ==============================================================================
# This file is explicitly invoked by `make ENV=prod up`.
# It uses the NVIDIA-optimized inference server.
# ==============================================================================

services:
  # --- CORE INFRASTRUCTURE ---
  db:
    image: postgres:16-alpine
    container_name: churninator_db_prod
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    # In prod, you might not expose the DB port to the public internet
    # ports:
    #   - "5432:5432"
    healthcheck: { test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"], interval: 10s, timeout: 5s, retries: 5 }

  redis:
    image: redis:7-alpine
    container_name: churninator_redis_prod
    healthcheck: { test: ["CMD", "redis-cli", "ping"], interval: 10s, timeout: 5s, retries: 5 }

  # --- APPLICATION SERVICES ---
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    container_name: churninator_api_prod
    command: uvicorn backend.src.main:app --host 0.0.0.0 --port 8000 # No --reload in prod
    depends_on: { db: { condition: service_healthy }, redis: { condition: service_healthy } }
    ports:
      - "8000:8000"
    env_file:
      - .env

  worker:
    build:
      context: .
      dockerfile: docker/Dockerfile.worker
    container_name: churninator_worker_prod
    depends_on: [redis, inference_server]
    env_file:
      - .env

  inference_server:
    build:
      context: .
      dockerfile: docker/Dockerfile.inference.nvidia # <-- Uses the NVIDIA Dockerfile
    container_name: churninator_inference_prod
    deploy: # <-- CRITICAL: This enables GPU access
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "8001:8001"
    volumes:
      - ./forge/checkpoints:/app/models
    env_file:
      - .env

  frontend:
    build:
      context: .
      dockerfile: docker/Dockerfile.frontend
    container_name: churninator_frontend_prod
    depends_on: [api]
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=http://localhost:8000 # This should be your public API URL in a real deployment

# --- NAMED VOLUMES ---
volumes:
  postgres_data:
