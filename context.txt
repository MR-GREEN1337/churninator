File: forge/__init__.py
-e
-e
-e
File: forge/utils/function_parser.py
# forge/utils/function_parser.py
# (This is the complete, correct code from the smoloperator context)
import re
from typing import Dict, List, Tuple, Any
from collections import OrderedDict
from pydantic import BaseModel

class FunctionCall(BaseModel):
    function_name: str; parameters: Dict[str, Any]; original_string: str; description: str = ""
    def to_string(self) -> str:
        if not self.parameters: return f"{self.function_name}()"
        positional_args, named_args = [], []
        for name, value in self.parameters.items():
            if name.startswith("arg_"): positional_args.append((int(name.split("_")[1]), value))
            else: named_args.append((name, value))
        positional_args.sort(key=lambda x: x[0])
        param_parts = [self._value_to_string(v) for _, v in positional_args]
        param_parts.extend(f"{n}={self._value_to_string(v)}" for n, v in named_args)
        return f"{self.function_name}({', '.join(param_parts)})"
    def _value_to_string(self, value: Any) -> str:
        if isinstance(value, str): return f"'{value}'"
        if isinstance(value, (list, tuple)): return f"[{', '.join(self._value_to_string(i) for i in value)}]"
        if isinstance(value, dict): return f"{{{', '.join(f'{self._value_to_string(k)}: {self._value_to_string(v)}' for k, v in value.items())}}}"
        if isinstance(value, bool): return str(value).lower()
        return str(value)

def parse_function_call(s: str) -> List[FunctionCall]:
    pattern = r'([a-zA-Z_][a-zA-Z0-9_.]*)\(([^)]*)\)'; matches = re.findall(pattern, s.strip()); results = []
    for match in matches:
        name, params_str = match[0], match[1]
        params = parse_parameters(params_str)
        results.append(FunctionCall(function_name=name, parameters=params, original_string=f"{name}({params_str})"))
    return results

def parse_parameters(params_str: str) -> Dict[str, Any]:
    if not params_str.strip(): return {}
    params = OrderedDict(); parts = split_parameters(params_str); pos_idx = 0
    for part in parts:
        part = part.strip()
        if not part: continue
        name, value = parse_single_parameter(part)
        if name.startswith("arg_"): name = f"arg_{pos_idx}"; pos_idx += 1
        params[name] = value
    return params

def split_parameters(params_str: str) -> List[str]:
    parts, current_part, depth, in_quotes = [], "", 0, False
    for char in params_str:
        if char in "\"'": in_quotes = not in_quotes
        elif not in_quotes and char == '(': depth += 1
        elif not in_quotes and char == ')': depth -= 1
        elif not in_quotes and char == ',' and depth == 0:
            parts.append(current_part.strip()); current_part = ""
            continue
        current_part += char
    if current_part.strip(): parts.append(current_part.strip())
    return parts

def parse_single_parameter(param_str: str) -> Tuple[str, Any]:
    match = re.match(r'^([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*(.+)$', param_str)
    if match: return match.group(1), parse_value(match.group(2).strip())
    return "arg_0", parse_value(param_str)

def parse_value(v_str: str) -> Any:
    v_str = v_str.strip()
    if (v_str.startswith("'") and v_str.endswith("'")) or \
       (v_str.startswith('"') and v_str.endswith('"')): return v_str[1:-1]
    if v_str.lower() == 'true': return True
    if v_str.lower() == 'false': return False
    try:
        if '.' in v_str: return float(v_str)
        return int(v_str)
    except ValueError: return v_str

def extract_function_calls_from_text(text: str) -> List[FunctionCall]:
    pattern = r'[a-zA-Z_][a-zA-Z0-9_.]*\([^)]*\)'
    matches = re.findall(pattern, text)
    results = []
    for m in matches: results.extend(parse_function_call(m))
    return results-e
-e
-e
File: forge/utils/__init__.py
-e
-e
-e
File: forge/utils/action_conversion.py
# forge/data/processing/action_conversion.py
# (This is the complete, correct code from the smoloperator context)
from forge.utils.function_parser import FunctionCall
from copy import deepcopy

def rename_parameters(action: FunctionCall):
    if not action.parameters: return
    new_params = {f"arg_{i}": v for i, v in enumerate(action.parameters.values())}
    action.parameters = new_params

def change_argument_name(action: FunctionCall):
    if "arg_0" in action.parameters:
        if isinstance(action.parameters["arg_0"], (list, tuple)): action.parameters["from_coord"] = tuple(float(c) for c in action.parameters["arg_0"])
        else: action.parameters["x"] = float(action.parameters["arg_0"])
        del action.parameters["arg_0"]
    if "arg_1" in action.parameters:
        if isinstance(action.parameters["arg_1"], (list, tuple)): action.parameters["to_coord"] = tuple(float(c) for c in action.parameters["arg_1"])
        else: action.parameters["y"] = float(action.parameters["arg_1"])
        del action.parameters["arg_1"]

def action_conversion(actions: list[FunctionCall], resolution: tuple[int, int]) -> list[FunctionCall]:
    for i, action in enumerate(actions):
        rename_parameters(action)
        name = action.function_name
        if name == "mobile.home": action.function_name = "navigate_home"
        elif name == "mobile.open_app": action.function_name = "open_app"
        elif name == "mobile.swipe": action.function_name = "swipe"; change_argument_name(action)
        elif name == "mobile.back": action.function_name = "navigate_back"
        elif name == "mobile.long_press": action.function_name = "long_press"; change_argument_name(action)
        elif name in ["mobile.terminate", "answer"]: action.function_name = "final_answer"
        elif name == "mobile.wait":
            action.function_name = "wait"
            if "arg_0" in action.parameters: action.parameters["seconds"] = int(action.parameters.pop("arg_0"))
        elif name == "pyautogui.click": action.function_name = "click"; change_argument_name(action)
        elif name == "pyautogui.doubleClick": action.function_name = "double_click"; change_argument_name(action)
        elif name == "pyautogui.rightClick": action.function_name = "right_click"; change_argument_name(action)
        elif name in ["pyautogui.hotkey", "pyautogui.press"]:
            action.function_name = "press"
            if "arg_0" in action.parameters: action.parameters["keys"] = action.parameters.pop("arg_0")
        elif name == "pyautogui.moveTo": action.function_name = "move_mouse"; change_argument_name(action)
        elif name == "pyautogui.write":
            action.function_name = "type"
            if "arg_0" in action.parameters: action.parameters["text"] = action.parameters.pop("arg_0")
        elif name in ["pyautogui.scroll", "pyautogui.hscroll"]:
            val = action.parameters.pop("arg_0")
            action.function_name = "scroll"
            action.parameters["amount"] = int(abs(val * 100))
            if name == "pyautogui.hscroll": action.parameters["direction"] = "left" if val < 0 else "right"
            else: action.parameters["direction"] = "up" if val < 0 else "down"
        elif name == "pyautogui.dragTo": action.function_name = "drag"; change_argument_name(action)
        action.original_string = action.to_string()
    return actions-e
-e
-e
File: forge/train/collator.py
# forge/training/collator.py
from typing import List, Dict, Any
from PIL import Image
import torch
from transformers import AutoProcessor

class FinetuneDataCollator:
    """
    A specialized data collator for the Churninator fine-tuning process.

    This collator takes a batch of data items from the FinetuneDataset and
    formats them into the precise structure expected by modern Vision-Language
    Models for training. It handles dynamic batching of images and text,
    padding, tokenization, and the crucial task of masking labels.
    """
    def __init__(self, processor: AutoProcessor, max_length: int = 4096):
        """
        Initializes the collator.

        Args:
            processor: The Hugging Face AutoProcessor for the model (e.g., for SmolLM/Idefics2).
            max_length (int): The maximum sequence length for the model input.
        """
        self.processor = processor
        self.max_length = max_length

    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """
        The main collating function that transforms a list of dataset items into a batch.

        Args:
            examples (List[Dict[str, Any]]): A list of dictionaries from our
                FinetuneDataset. Each item has 'image' (a PIL Image) and 'messages'
                (a list of conversation turn dictionaries).

        Returns:
            Dict[str, torch.Tensor]: A dictionary of batched and padded tensors
                ready for the model, including 'input_ids', 'attention_mask',
                pixel_values, and 'labels'.
        """

        # Extract images and message lists from the batch
        images = [example["image"] for example in examples]
        batch_messages = [example["messages"] for example in examples]

        # Use the processor's chat template to convert the structured message lists
        # into a single, correctly formatted prompt string for each example.
        # This handles all the special tokens (e.g., <|user|>, <|assistant|>) automatically.
        full_texts = [
            self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=False
            )
            for messages in batch_messages
        ]

        # Tokenize the text and process the images all at once.
        # This is the most efficient method and correctly handles multimodal inputs.
        batch = self.processor(
            text=full_texts,
            images=images,
            padding="longest", # Pad to the longest sequence in the batch
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt",
        )

        # --- LABEL MASKING ---
        # Our goal is to train the model *only* on the assistant's responses.
        # We do this by creating a 'labels' tensor where all tokens belonging to
        # the user or system prompts are masked with -100.

        labels = batch["input_ids"].clone()

        # To find where the assistant's part begins, we can process the prompts
        # again, but this time *without* the assistant's final response.
        prompts_without_assistant = []
        for messages in batch_messages:
            # Create a version of the conversation with the last (assistant) message removed
            prompt_only_messages = messages[:-1]
            prompt_text = self.processor.apply_chat_template(
                prompt_only_messages, tokenize=False, add_generation_prompt=False
            )
            prompts_without_assistant.append(prompt_text)

        # Tokenize these "prompt-only" texts to find their length
        prompt_lengths = self.processor(
            text=prompts_without_assistant,
            padding=False, # We only need the length, not padding
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt",
        )["input_ids"].shape[1]

        # For each item in the batch, mask the prompt part
        for i in range(len(examples)):
            prompt_len = prompt_lengths[i]
            # Set all tokens up to the end of the prompt to -100
            labels[i, :prompt_len] = -100
            # Also mask any padding tokens at the end
            # The attention mask is 1 for real tokens and 0 for padding.
            labels[i][batch['attention_mask'][i] == 0] = -100


        batch["labels"] = labels
        return batch-e
-e
-e
File: forge/train/datasets.py
# forge/train/datasets.py
import json
from pathlib import Path
import torch
from torch.utils.data import Dataset
from PIL import Image

class FinetuneDataset(Dataset):
    """
    Loads and processes data for both Stage 1 (Grounding) and Stage 2 (Reasoning)
    of the Churninator fine-tuning process.
    """
    def __init__(self, jsonl_file: str, processor, raw_data_root: str, stage: int):
        """
        Initializes the dataset.

        Args:
            jsonl_file (str): Path to the processed .jsonl data file.
            processor: The Hugging Face model processor.
            raw_data_root (str): Path to the root of the raw data directory (for resolving image paths).
            stage (int): The training stage (1 or 2) to configure the prompt format.
        """
        self.data = [json.loads(line) for line in open(jsonl_file)]
        self.processor = processor
        self.raw_data_root = Path(raw_data_root)
        self.stage = stage

        if self.stage not in [1, 2]:
            raise ValueError(f"Invalid stage specified: {self.stage}. Must be 1 or 2.")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx: int):
        item = self.data[idx]

        # Resolve the full image path
        image_path = self.raw_data_root / item['image_path']
        try:
            image = Image.open(image_path).convert("RGB")
        except Exception as e:
            print(f"Warning: Could not load image at {image_path}. Error: {e}. Skipping.")
            # Recursively try the next item if the current one is broken
            return self.__getitem__((idx + 1) % len(self))

        # --- DYNAMIC PROMPT FORMATTING based on stage ---
        if self.stage == 1:
            # Stage 1: Simple Instruction -> Action for Grounding
            messages = [
                {"role": "system", "content": "You are a helpful GUI agent. You will be given an instruction and a screenshot. Respond with the single pyautogui-style action to complete the instruction."},
                {"role": "user", "content": f"Instruction: {item['instruction']}"},
                {"role": "assistant", "content": f"Action: {item['action']}"}
            ]

        elif self.stage == 2:
            # Stage 2: Instruction -> Thought & Action for Reasoning
            # This format teaches the model the "inner monologue" structure.
            # Assumes the preprocessed data for stage 2 has 'thought' and 'action' keys.
            thought = item.get("thought", "I need to analyze the screen and decide the next step.") # Default thought
            action = item.get("action", "TERMINATE('missing action')")

            assistant_response = f"<think>{thought}</think>\n<code>{action}</code>"
            messages = [
                {"role": "system", "content": "You are a helpful GUI agent. First, think step-by-step about your plan inside <think> tags. Then, provide the single action to perform inside <code> tags."},
                {"role": "user", "content": f"Instruction: {item['instruction']}"},
                {"role": "assistant", "content": assistant_response}
            ]

        # Return the raw image and messages; the collator will handle tokenization.
        return {"image": image, "messages": messages}-e
-e
-e
File: forge/train/__init__.py
-e
-e
-e
File: forge/train/train.py
# forge/train/train.py
import yaml
import os
from pathlib import Path
import torch
from transformers import (
    AutoProcessor,
    Idefics2ForConditionalGeneration,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, PeftModel
import wandb

from forge.train.datasets import FinetuneDataset
from forge.train.collator import FinetuneDataCollator

def main(config_path: str):
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    # --- W&B Setup ---
    wandb.init(project="TheChurninatorForge", name=Path(config['output_dir']).name, config=config)

    # --- Model & Processor Loading ---
    print(f"Loading base model config from: {config['base_model']}")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    base_model_id = "HuggingFaceTB/SmolLM-1.7B-32k-instruct"
    processor = AutoProcessor.from_pretrained(base_model_id)

    model = Idefics2ForConditionalGeneration.from_pretrained(
        base_model_id,
        quantization_config=quantization_config,
        device_map="auto"
    )

    # --- DYNAMIC MODEL SETUP for Stage 1 vs Stage 2 ---
    if os.path.isdir(config['base_model']):
        print(f"--- STAGE 2 RUN DETECTED ---")
        print(f"Loading Stage 1 LoRA adapters from: {config['base_model']}")
        model = PeftModel.from_pretrained(model, config['base_model'])
        print("Successfully loaded Stage 1 adapters.")
    else:
        print("--- STAGE 1 RUN DETECTED ---")

    # --- LoRA Configuration ---
    lora_config = LoraConfig(
        r=config['lora_r'],
        lora_alpha=config['lora_alpha'],
        lora_dropout=config['lora_dropout'],
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        use_rslora=True,
        bias="none"
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # --- Dataset Loading ---
    stage_num = 2 if 'reasoning' in config['data_file'] else 1
    print(f"Configuring dataset for training Stage {stage_num}.")

    data_file = Path("forge/data/processed") / config['data_file']
    raw_data_root = Path("forge/data/raw")
    train_dataset = FinetuneDataset(data_file, processor, raw_data_root, stage=stage_num)

    # --- Data Collator ---
    data_collator = FinetuneDataCollator(processor)

    # --- Training Arguments ---
    training_args = TrainingArguments(
        output_dir=config['output_dir'],
        num_train_epochs=config['num_train_epochs'],
        per_device_train_batch_size=config['per_device_train_batch_size'],
        gradient_accumulation_steps=config['gradient_accumulation_steps'],
        gradient_checkpointing=config['gradient_checkpointing'],
        learning_rate=config['learning_rate'],
        optim=config['optim'],
        logging_steps=config['logging_steps'],
        save_strategy="epoch",
        report_to="wandb",
        bf16=config.get('bf16', False),
        fp16=config.get('fp16', False),
        remove_unused_columns=False,
    )

    # --- Initialize Trainer ---
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )

    # --- START TRAINING ---
    print("ğŸ”¥ğŸ”¥ğŸ”¥ The Forge is lit. Fine-tuning has begun. ğŸ”¥ğŸ”¥ğŸ”¥")
    trainer.train()
    print("âœ… Training complete!")

    # --- Save Final Model ---
    final_path = Path(config['output_dir']) / "final"
    trainer.save_model(str(final_path))
    print(f"âœ… Final model adapters saved to {final_path}")
    wandb.finish()

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", required=True, help="Path to training config YAML.")
    args = parser.parse_args()
    main(args.config)-e
-e
-e
File: forge/eval/__init__.py
-e
-e
-e
File: forge/eval/eval_prompt.py
# forge/eval/eval_prompt.py
SCREENSPOT_V2_USER_PROMPT_PHASE_1 = """Using the screenshot, you will get an instruction and will need to output the action that completes the instruction or targets the given element.

Just write your action as follows:

Action: click(x=0.XXXX, y=0.YYYY)
With 0.XXXX and 0.YYYY the normalized coordinates of the click position on the screenshot.

Now write the action needed to complete the instruction:
Instruction: {instruction}
"""-e
-e
-e
File: forge/eval/eval_dataset.py
# forge/eval/eval_dataset.py
import json
from pathlib import Path
from torch.utils.data import Dataset
from PIL import Image

class ScreenSpotDataset(Dataset):
    """Loads the ScreenSpot benchmark dataset for quantitative evaluation."""
    def __init__(self, data_root: str, split: str = "all"):
        self.data_root = Path(data_root)
        self.split = split
        self.data = self._load_data()

    def _load_data(self):
        all_data = []
        # ScreenSpot has web, mobile, desktop splits
        splits_to_load = ["web", "mobile", "desktop"] if self.split == "all" else [self.split]
        for s in splits_to_load:
            json_path = self.data_root / f"annotations/{s}.json"
            with open(json_path, 'r') as f:
                data = json.load(f)
                for item in data:
                    item['split'] = s
                    all_data.append(item)
        return all_data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        image_path = self.data_root / f"screenshot/{item['split']}/{item['image']}"
        try:
            image = Image.open(image_path).convert("RGB")
        except Exception:
            # Fallback for corrupted data
            return self.__getitem__((idx + 1) % len(self))

        # Ground truth bounding box (left, top, right, bottom)
        bbox = [
            item['bbox'][0],
            item['bbox'][1],
            item['bbox'][0] + item['bbox'][2],
            item['bbox'][1] + item['bbox'][3],
        ]

        return {
            "image": image,
            "instruction": item['instruction'],
            "ground_truth_bbox": bbox
        }-e
-e
-e
File: forge/eval/eval.py
# forge/eval/eval.py
import argparse
import torch
from transformers import AutoProcessor, Idefics2ForConditionalGeneration
from peft import PeftModel
from PIL import Image
import requests
from pathlib import Path
from tqdm import tqdm
import sys

# Add project root to path for imports
sys.path.append(str(Path(__file__).resolve().parents[2]))

from forge.eval.eval_prompt import SCREENSPOT_V2_USER_PROMPT_PHASE_1
from forge.utils.function_parser import extract_function_calls_from_text
from forge.eval.eval_dataset import ScreenSpotDataset

def load_model_for_eval(model_path: str):
    """Loads a fine-tuned PEFT model and merges it for fast inference."""
    print(f"--- Loading model from: {model_path} ---")
    base_model_id = "HuggingFaceTB/SmolLM-1.7B-32k-instruct"

    # Use float16 for Mac, bfloat16 for CUDA
    dtype = torch.float16 if torch.backends.mps.is_available() else torch.bfloat16

    model = Idefics2ForConditionalGeneration.from_pretrained(base_model_id, torch_dtype=dtype)
    model = PeftModel.from_pretrained(model, model_path)
    model = model.merge_and_unload() # Merge adapters for fast inference

    device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    processor = AutoProcessor.from_pretrained(base_model_id)
    print(f"âœ… Model loaded to {device}.")
    return model, processor

def run_qualitative_test(model, processor, image_url, instruction):
    """Runs a single, qualitative spot-check on the model."""
    print("\n--- Running Qualitative Spot-Check ---")
    print(f"Fetching image from: {image_url}")
    try:
        image = Image.open(requests.get(image_url, stream=True).raw).convert("RGB")
    except Exception as e:
        print(f"âŒ Failed to load image: {e}")
        return

    prompt_text = SCREENSPOT_V2_USER_PROMPT_PHASE_1.format(instruction=instruction)
    messages = [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": prompt_text}]}]
    inputs = processor(messages, image=image, return_tensors="pt").to(model.device)

    print("ğŸ¤– Generating prediction...")
    generated_ids = model.generate(**inputs, max_new_tokens=50)
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    print("\n--- RESULTS ---")
    print(f"Instruction: {instruction}")
    print(f"Full Model Output:\n{generated_text}")

    parsed_actions = extract_function_calls_from_text(generated_text)
    if parsed_actions:
        print(f"\nâœ… Parsed Action: {parsed_actions[0].to_string()}")
    else:
        print(f"\nâš ï¸ Could not parse an action from the model's output.")

def check_click_accuracy(pred_x, pred_y, gt_bbox):
    """Checks if a predicted click (normalized) is within a ground truth bbox (pixels)."""
    # The bbox is [x_min, y_min, x_max, y_max] in pixels.
    # The prediction is normalized. We assume image size 1x1 for simplicity.
    return gt_bbox[0] <= pred_x <= gt_bbox[2] and gt_bbox[1] <= pred_y <= gt_bbox[3]

def run_quantitative_benchmark(model, processor, data_root, limit=100):
    """Runs a quantitative benchmark on the ScreenSpot dataset."""
    print("\n--- Running Quantitative Benchmark on ScreenSpot ---")
    dataset = ScreenSpotDataset(data_root=data_root)

    correct_predictions = 0
    total_predictions = 0

    # Limit the number of samples for a quick benchmark
    dataset_subset = torch.utils.data.Subset(dataset, range(min(limit, len(dataset))))

    for item in tqdm(dataset_subset, desc="Benchmarking"):
        image = item['image']
        instruction = item['instruction']
        gt_bbox_pixels = item['ground_truth_bbox']

        # We need to normalize the ground truth bbox to compare with model output
        img_width, img_height = image.size
        gt_bbox_norm = [
            gt_bbox_pixels[0] / img_width,
            gt_bbox_pixels[1] / img_height,
            gt_bbox_pixels[2] / img_width,
            gt_bbox_pixels[3] / img_height,
        ]

        prompt_text = SCREENSPOT_V2_USER_PROMPT_PHASE_1.format(instruction=instruction)
        messages = [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": prompt_text}]}]
        inputs = processor(messages, image=image, return_tensors="pt").to(model.device)

        generated_ids = model.generate(**inputs, max_new_tokens=50)
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

        parsed_actions = extract_function_calls_from_text(generated_text)
        total_predictions += 1

        if parsed_actions and parsed_actions[0].function_name == "click":
            params = parsed_actions[0].parameters
            if 'x' in params and 'y' in params:
                pred_x, pred_y = params['x'], params['y']
                if check_click_accuracy(pred_x, pred_y, gt_bbox_norm):
                    correct_predictions += 1

    accuracy = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0
    print("\n--- BENCHMARK COMPLETE ---")
    print(f"Total Samples: {total_predictions}")
    print(f"Correct Predictions: {correct_predictions}")
    print(f"Accuracy: {accuracy:.2f}%")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Churninator Model Evaluation Suite")
    parser.add_argument("--model-path", required=True, help="Path to the fine-tuned PEFT model checkpoint directory (e.g., forge/checkpoints/run/final).")
    parser.add_argument("--eval-type", required=True, choices=['qualitative', 'quantitative'], help="Type of evaluation to run.")

    # Args for qualitative eval
    parser.add_argument("--image-url", help="URL of the image for qualitative test.")
    parser.add_argument("--instruction", help="Instruction for qualitative test.")

    # Args for quantitative eval
    parser.add_argument("--data-root", default="forge/data/eval_data/ScreenSpot_v2", help="Root directory of the ScreenSpot evaluation data.")
    parser.add_argument("--limit", type=int, default=100, help="Number of samples to run for quantitative benchmark.")

    args = parser.parse_args()

    model, processor = load_model_for_eval(args.model_path)

    if args.eval-type == 'qualitative':
        if not args.image_url or not args.instruction:
            raise ValueError("--image-url and --instruction are required for qualitative evaluation.")
        run_qualitative_test(model, processor, args.image_url, args.instruction)
    elif args.eval-type == 'quantitative':
        run_quantitative_benchmark(model, processor, args.data_root, args.limit)-e
-e
-e
File: forge/data/preprocess.py
# forge/data/preprocess.py
import json
from pathlib import Path
from datasets import load_dataset
from tqdm import tqdm
import logging

from forge.utils.function_parser import parse_function_call
from forge.utils.action_conversion import action_conversion

# Setup basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def process_stage(stage_name, stage_num, raw_data_dir, output_dir):
    """Processes a single stage of the AGUVIS dataset, now with action conversion."""
    raw_stage_path = raw_data_dir / f"aguvis-stage{stage_num}"
    if not raw_stage_path.exists():
        logging.error(f"Raw data path not found: {raw_stage_path}")
        return

    logging.info(f"--- Starting processing for {stage_name} ---")

    dataset = load_dataset(str(raw_stage_path), name='default', split='train')
    output_file = output_dir / f"stage{stage_num}_{stage_name.lower()}.jsonl"
    logging.info(f"Processing and saving to {output_file}...")

    count = 0
    conversion_errors = 0
    with open(output_file, "w") as f:
        for item in tqdm(dataset, desc=f"Processing {stage_name}"):
            try:
                # --- ACTION CONVERSION LOGIC ---
                # 1. Parse the raw action string using our universal translator
                original_action_str = item["action"]
                parsed_calls = parse_function_call(original_action_str)

                if not parsed_calls:
                    continue # Skip if the action is malformed

                # 2. Convert to our clean, unified action space using the AGUVIS specialist
                # We assume a dummy resolution for now, as the model will learn normalized coords.
                # The actual pixel values don't matter during this conversion step.
                converted_calls = action_conversion(parsed_calls, resolution=(1, 1))

                # 3. Reconstruct the clean action string
                clean_action_str = " ".join([call.to_string() for call in converted_calls])
                # --- END ACTION CONVERSION ---

                portable_image_path = str(Path(f"aguvis-stage{stage_num}") / item["image"])

                processed_item = {
                    "image_path": portable_image_path,
                    "instruction": item["instruction"],
                    "action": clean_action_str, # Use the clean, converted action
                }
                f.write(json.dumps(processed_item) + "\n")
                count += 1
            except Exception as e:
                conversion_errors += 1
                logging.warning(f"Skipping record due to conversion error: {e} on action '{item.get('action', 'N/A')}'")

    logging.info(f"âœ… {stage_name} processing complete. Wrote {count} records to {output_file}")
    if conversion_errors > 0:
        logging.warning(f"âš ï¸ Encountered {conversion_errors} action conversion errors.")

def main():
    raw_data_dir = Path("forge/data/raw")
    output_dir = Path("forge/data/processed")
    output_dir.mkdir(parents=True, exist_ok=True)

    process_stage("grounding", 1, raw_data_dir, output_dir)
    # You can uncomment this later when you're ready for Stage 2
    # process_stage("reasoning", 2, raw_data_dir, output_dir)

if __name__ == "__main__":
    main()-e
-e
-e
File: forge/data/processing/__init__.py
-e
-e
-e
File: forge/data/__init__.py
-e
-e
-e
File: backend/main.py
def main():
    print("Hello from backend!")


if __name__ == "__main__":
    main()
-e
-e
-e
File: web/next-env.d.ts
/// <reference types="next" />
/// <reference types="next/image-types/global" />
/// <reference path="./.next/types/routes.d.ts" />

// NOTE: This file should not be edited
// see https://nextjs.org/docs/app/api-reference/config/typescript for more information.
-e
-e
-e
File: web/next.config.ts
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;
-e
-e
-e
File: web/src/app/layout.tsx
import type { Metadata } from "next";
import { Geist, Geist_Mono } from "next/font/google";
import "./globals.css";

const geistSans = Geist({
  variable: "--font-geist-sans",
  subsets: ["latin"],
});

const geistMono = Geist_Mono({
  variable: "--font-geist-mono",
  subsets: ["latin"],
});

export const metadata: Metadata = {
  title: "Create Next App",
  description: "Generated by create next app",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased`}
      >
        {children}
      </body>
    </html>
  );
}
-e
-e
-e
File: web/src/app/page.tsx
import Image from "next/image";

export default function Home() {
  return (
    <div className="font-sans grid grid-rows-[20px_1fr_20px] items-center justify-items-center min-h-screen p-8 pb-20 gap-16 sm:p-20">
      <main className="flex flex-col gap-[32px] row-start-2 items-center sm:items-start">
        <Image
          className="dark:invert"
          src="/next.svg"
          alt="Next.js logo"
          width={180}
          height={38}
          priority
        />
        <ol className="font-mono list-inside list-decimal text-sm/6 text-center sm:text-left">
          <li className="mb-2 tracking-[-.01em]">
            Get started by editing{" "}
            <code className="bg-black/[.05] dark:bg-white/[.06] font-mono font-semibold px-1 py-0.5 rounded">
              src/app/page.tsx
            </code>
            .
          </li>
          <li className="tracking-[-.01em]">
            Save and see your changes instantly.
          </li>
        </ol>

        <div className="flex gap-4 items-center flex-col sm:flex-row">
          <a
            className="rounded-full border border-solid border-transparent transition-colors flex items-center justify-center bg-foreground text-background gap-2 hover:bg-[#383838] dark:hover:bg-[#ccc] font-medium text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5 sm:w-auto"
            href="https://vercel.com/new?utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app"
            target="_blank"
            rel="noopener noreferrer"
          >
            <Image
              className="dark:invert"
              src="/vercel.svg"
              alt="Vercel logomark"
              width={20}
              height={20}
            />
            Deploy now
          </a>
          <a
            className="rounded-full border border-solid border-black/[.08] dark:border-white/[.145] transition-colors flex items-center justify-center hover:bg-[#f2f2f2] dark:hover:bg-[#1a1a1a] hover:border-transparent font-medium text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5 w-full sm:w-auto md:w-[158px]"
            href="https://nextjs.org/docs?utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app"
            target="_blank"
            rel="noopener noreferrer"
          >
            Read our docs
          </a>
        </div>
      </main>
      <footer className="row-start-3 flex gap-[24px] flex-wrap items-center justify-center">
        <a
          className="flex items-center gap-2 hover:underline hover:underline-offset-4"
          href="https://nextjs.org/learn?utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app"
          target="_blank"
          rel="noopener noreferrer"
        >
          <Image
            aria-hidden
            src="/file.svg"
            alt="File icon"
            width={16}
            height={16}
          />
          Learn
        </a>
        <a
          className="flex items-center gap-2 hover:underline hover:underline-offset-4"
          href="https://vercel.com/templates?framework=next.js&utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app"
          target="_blank"
          rel="noopener noreferrer"
        >
          <Image
            aria-hidden
            src="/window.svg"
            alt="Window icon"
            width={16}
            height={16}
          />
          Examples
        </a>
        <a
          className="flex items-center gap-2 hover:underline hover:underline-offset-4"
          href="https://nextjs.org?utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app"
          target="_blank"
          rel="noopener noreferrer"
        >
          <Image
            aria-hidden
            src="/globe.svg"
            alt="Globe icon"
            width={16}
            height={16}
          />
          Go to nextjs.org â†’
        </a>
      </footer>
    </div>
  );
}
-e
-e
-e
File: ./.pre-commit-config.yaml
# ==============================================================================
# Global Hooks - Apply to all files in the repository
# ==============================================================================
repos:
-   repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.6.0
    hooks:
    -   id: check-added-large-files
        args: ['--maxkb=2048'] # No files > 2MB (models should be gitignored)
    -   id: check-case-conflict
    -   id: check-merge-conflict
    -   id: check-yaml
    -   id: end-of-file-fixer
    -   id: trailing-whitespace

-   repo: https://github.com/Yelp/detect-secrets
    rev: v1.5.0
    hooks:
    -   id: detect-secrets
        args: ['--baseline', '.secrets.baseline'] # Generate baseline first

# ==============================================================================
# Python Hooks - Apply to `backend/` and `forge/` directories
# ==============================================================================
-   repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.4.4
    hooks:
    # Run the linter
    -   id: ruff
        args: [--fix, --exit-non-zero-on-fix]
        files: ^(backend|forge)/
    # Run the formatter
    -   id: ruff-format
        files: ^(backend|forge)/

-   repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.10.0
    hooks:
    -   id: mypy
        files: ^(backend|forge)/
        args: [--ignore-missing-imports]
        # Mypy can be slow, so you might want to run it manually or in CI
        # To enable it, uncomment the line above.
        # additional_dependencies: [
        #    "pydantic", "fastapi", "celery", "redis",
        #    "torch", "transformers", "datasets"
        # ]

# ==============================================================================
# Frontend Hooks - Apply to `web/` directory
# ==============================================================================
-   repo: https://github.com/pre-commit/mirrors-prettier
    rev: v3.1.0
    hooks:
    -   id: prettier
        files: ^web/.*\.(js|jsx|ts|tsx|css|scss|md|json)$
        args: ['--write'] # Auto-format files

-   repo: https://github.com/pre-commit/mirrors-eslint
    rev: ''
    hooks:
    -   id: eslint
        files: ^web/.*\.(js|jsx|ts|tsx)$
        args: [--fix]
        additional_dependencies:
        -   eslint@8.57.0
        -   typescript@5.4.5
        -   '@typescript-eslint/parser@7.9.0'
        -   '@typescript-eslint/eslint-plugin@7.9.0'
        -   'eslint-plugin-react@7.34.1'
        -   'eslint-config-next@14.2.3'

# ==============================================================================
# Frontend Type Checking - A special case
# ==============================================================================
-   repo: local
    hooks:
    -   id: tsc-no-emit
        name: Run tsc --noEmit
        entry: bash -c 'cd web && npm run typecheck || exit 1'
        language: system
        files: ^web/.*\.(ts|tsx)$
-e
-e
-e
File: ./claude_context.sh
#!/bin/bash
#
# Churninator - Context Generation Script v2.0 (Final)
# Gathers all RELEVANT source, configs, and scripts, with robust exclusions.
#

echo "--- Generating complete context for The Churninator ---"

# --- Step 1: Clear previous context ---
echo "[1/5] Clearing old context file..."
> context.txt

# --- Step 2: Append Forge & Backend Source (Python) ---
echo "[2/5] Appending Forge & Backend source files (*.py)..."
find forge backend -type f -name "*.py" \
  -not -path "*/.venv/*" \
  -not -path "*/__pycache__/*" \
  -exec sh -c '
  echo "File: {}" >> context.txt && cat {} >> context.txt && echo -e "\n-e \n-e" >> context.txt
' \;

# --- Step 3: Append Frontend Source (Next.js) ---
echo "[3/5] Appending Frontend source files (*.ts, *.tsx)..."
find web -type f \( -name "*.ts" -o -name "*.tsx" \) \
  -not -path "*/node_modules/*" \
  -not -path "*/.next/*" \
  -exec sh -c '
  echo "File: $1" >> context.txt && cat "$1" >> context.txt && echo -e "\n-e \n-e" >> context.txt
' sh {} \;

# --- Step 4: Append Configs & Scripts (YAML & Shell) ---
echo "[4/5] Appending Configs (*.yaml) & Scripts (*.sh)..."
# THE FIX IS HERE: We now exclude .venv, .git, and node_modules from this global find.
find . -type f \( -name "*.yaml" -o -name "*.yml" -o -name "*.sh" \) \
  -not -path "./.git/*" \
  -not -path "*/.venv/*" \
  -not -path "*/node_modules/*" \
  -not -path "./forge/data/raw/*" \
  -exec sh -c '
  echo "File: {}" >> context.txt && cat {} >> context.txt && echo -e "\n-e \n-e" >> context.txt
' \;

# --- Step 5: Append Directory Trees & Final Prompt ---
echo "[5/5] Appending directory trees and project prompt..."
{
  echo "--- DIRECTORY TREES ---"
  echo ""
  echo "Forge Tree:"
  tree -I '.venv|__pycache__|data/raw|checkpoints|*.egg-info' forge
  echo ""
  echo "Backend Tree:"
  tree -I '.venv|__pycache__|.pytest_cache' backend
  echo ""
  echo "Frontend Tree (web/):"
  tree -I 'node_modules|.next|out' web
  echo ""
  echo "-----------------------"
  echo ""
} >> context.txt

# Append your startup context at the bottom
cat <<'EOT' >> context.txt
Project Context: The Churninator - Autonomous AI Mystery Shopper for SaaS

Core Concept: An open-source, AI-powered platform designed to autonomously analyze the signup and onboarding experience of any web application. It acts as a "mystery shopper," identifying UX friction points, conversion killers, and usability issues, then presents the findings in a detailed, interactive report.

System Architecture: Multi-Component, Cloud-Native Platform

1. The Forge (forge/):
Role: The "AI Workshop & ML Pipeline."
Function: Contains all scripts for the two-stage AGUVIS fine-tuning methodology (Stage 1: Grounding, Stage 2: Reasoning). It's designed for full-scale training on remote cloud GPUs (like Vast.ai).

2. The Backend (backend/):
Role: The "Mission Control & Production Engine."
Sub-components:
- API / Control Plane (backend/api/): FastAPI server for user requests and job queuing (Redis).
- Agent Worker (backend/worker/): Scalable Celery workers running Playwright in a virtual display (Xvfb).
- Inference Server (backend/inference/): Hosts the fine-tuned Churninator model (e.g., using vLLM), acting as the remote brain for the workers.

3. The Frontend (web/):
Role: The "Observatory & Executive Dashboard."
Function: A Next.js application with a "live view" dashboard (WebSockets for logs, MJPEG for video) to observe agents in real-time.

User Experience & Core Loop:
A user submits a URL and task. The API queues a job. A Worker picks it up, streams its browser view and "thoughts" to the frontend, and generates a final report on all discovered UX issues.
EOT

echo "--- Context generation complete. File 'context.txt' is ready. ---"-e
-e
-e
File: ./forge/config.yaml
-e
-e
-e
File: ./forge/train/config/stage1_grounding.yaml
# --- Model & Data ---
base_model: "HuggingFaceM4/Idefics2-8b-base"
data_file: "stage1_grounding.jsonl" # This will be found in the processed data directory
output_dir: "checkpoints/churninator-idefics2-8b-grounded-v1" # Where to save the model adapters

# --- Training Hyperparameters ---
training_mode: "lora"
lora_r: 32
lora_alpha: 64
lora_dropout: 0.1

learning_rate: 1.0e-4
num_train_epochs: 1
per_device_train_batch_size: 2
gradient_accumulation_steps: 16 # Effective batch size of 32
gradient_checkpointing: true
optim: "paged_adamw_8bit" # Use paged AdamW for memory efficiency

# --- Precision & Hardware ---
bf16: true # Use bfloat16 for modern GPUs (A100, H100, 4090)
fp16: false # Set to true if using older GPUs like V100 or 3090

# --- Logging ---
report_to: "wandb"
logging_steps: 20-e
-e
-e
File: ./forge/train/config/stage2_reasoning.yaml
# --- Model & Data ---
# We are NOT loading the base model. We are starting from our Stage 1 checkpoint.
base_model: "forge/checkpoints/churninator-smollm-1.7b-grounded-v1/final" # <-- CRITICAL: Path to your Stage 1 model
data_file: "stage2_reasoning.jsonl"
output_dir: "checkpoints/churninator-smollm-1.7b-reasoning-v1" # Final model output

# --- Training Hyperparameters ---
# We often use a slightly lower learning rate for the second stage of tuning.
training_mode: "lora"
lora_r: 32
lora_alpha: 64
lora_dropout: 0.05

learning_rate: 5.0e-5 # Lower LR for fine-tuning an already-tuned model
num_train_epochs: 1
per_device_train_batch_size: 1 # Reasoning data has longer sequences
gradient_accumulation_steps: 16
gradient_checkpointing: true
optim: "paged_adamw_8bit"

# --- Precision & Logging ---
bf16: true
fp16: false
report_to: "wandb"
logging_steps: 10-e
-e
-e
File: ./forge/data/download_eval_data.sh
#!/bin/bash
# This script downloads the ScreenSpot benchmark dataset for evaluation.
# Run from the project root: bash forge/data/download_eval_data.sh

set -e

EVAL_DATA_DIR="forge/data/eval_data"
REPO="https://huggingface.co/datasets/HongxinLi/ScreenSpot_v2"

echo "ğŸ”¥ Downloading ScreenSpot_v2 evaluation dataset..."
echo "Target directory: $EVAL_DATA_DIR"

mkdir -p "$EVAL_DATA_DIR"
cd "$EVAL_DATA_DIR"

if [ -d "ScreenSpot_v2" ]; then
    echo "ScreenSpot_v2 directory already exists. Skipping download."
else
    GIT_LFS_SKIP_SMUDGE=1 git clone "$REPO" ScreenSpot_v2
    cd ScreenSpot_v2
    git lfs pull
    cd ..
    echo "âœ… ScreenSpot_v2 download complete."
fi

cd ../../.. # Return to project root-e
-e
-e
File: ./forge/data/download_aguvis.sh
#!/bin/bash
# This script downloads the full, raw AGUVIS datasets from the Hugging Face Hub.
# It should be run ON THE REMOTE (Vast.ai) MACHINE.
# Requires git and git-lfs to be installed.

set -e

# --- Configuration ---
RAW_DATA_DIR="./raw" # Download into a 'raw' subdirectory
STAGE1_REPO="https://huggingface.co/datasets/xlangai/aguvis-stage1"
STAGE2_REPO="https://huggingface.co/datasets/xlangai/aguvis-stage2"

# --- Main Logic ---
echo "ğŸ”¥ Starting FULL AGUVIS dataset download..."
echo "Target directory: $RAW_DATA_DIR"

mkdir -p "$RAW_DATA_DIR"
cd "$RAW_DATA_DIR"

echo "--------------------------------------------------"
echo "Downloading Stage 1: Grounding Dataset..."
if [ -d "aguvis-stage1" ]; then
    echo "Stage 1 directory already exists. Skipping."
else
    GIT_LFS_SKIP_SMUDGE=1 git clone "$STAGE1_REPO"
    cd aguvis-stage1 && git lfs pull && cd ..
    echo "âœ… Stage 1 download complete."
fi

echo "--------------------------------------------------"
echo "Downloading Stage 2: Reasoning Dataset..."
if [ -d "aguvis-stage2" ]; then
    echo "Stage 2 directory already exists. Skipping."
else
    GIT_LFS_SKIP_SMUDGE=1 git clone "$STAGE2_REPO"
    cd aguvis-stage2 && git lfs pull && cd ..
    echo "âœ… Stage 2 download complete."
fi

echo "âœ… All raw materials are now on the remote machine."-e
-e
-e
File: ./scripts/sync_from_vast.sh
#!/bin/bash

# --- Configuration ---
REMOTE_USER="root"
REMOTE_HOST="<IP_ADDRESS>"
REMOTE_PORT="<PORT>"

# The remote directory with your results
REMOTE_DIR="/workspace/forge/checkpoints/"
# The local destination
LOCAL_DIR="./forge/checkpoints/"

echo "ğŸ”½ Syncing results from the remote machine..."

# Create the local directory if it doesn't exist
mkdir -p "$LOCAL_DIR"

rsync -avz \
  -e "ssh -p $REMOTE_PORT" \
  "$REMOTE_USER@$REMOTE_HOST:$REMOTE_DIR" \
  "$LOCAL_DIR"

echo "âœ… Results sync complete."-e
-e
-e
File: ./scripts/sync_to_vast.sh
#!/bin/bash

# --- Configuration ---
REMOTE_USER="root"
REMOTE_HOST="<IP_ADDRESS>"
REMOTE_PORT="<PORT>"

# The remote directory with your results
REMOTE_DIR="/workspace/forge/checkpoints/"
# The local destination
LOCAL_DIR="./forge/checkpoints/"

echo "ğŸ”½ Syncing results from the remote machine..."

# Create the local directory if it doesn't exist
mkdir -p "$LOCAL_DIR"

rsync -avz \
  -e "ssh -p $REMOTE_PORT" \
  "$REMOTE_USER@$REMOTE_HOST:$REMOTE_DIR" \
  "$LOCAL_DIR"

echo "âœ… Results sync complete."-e
-e
-e
File: ./docker-compose.yml
-e
-e
-e
--- DIRECTORY TREES ---

Forge Tree:
forge
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.yaml
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ download_aguvis.sh
â”‚Â Â  â”œâ”€â”€ download_eval_data.sh
â”‚Â Â  â”œâ”€â”€ eval_data
â”‚Â Â  â”‚Â Â  â””â”€â”€ ScreenSpot_v2
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ data
â”‚Â Â  â”‚Â Â      â”‚Â Â  â”œâ”€â”€ test-00000-of-00002.parquet
â”‚Â Â  â”‚Â Â      â”‚Â Â  â””â”€â”€ test-00001-of-00002.parquet
â”‚Â Â  â”‚Â Â      â””â”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ preprocess.py
â”‚Â Â  â”œâ”€â”€ processing
â”‚Â Â  â”‚Â Â  â””â”€â”€ __init__.py
â”‚Â Â  â””â”€â”€ raw
â”‚Â Â      â”œâ”€â”€ aguvis-stage1
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ guienv.json
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ guienvs.zip
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ omniact_fix.json
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ omniact.zip
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ ricoig16k.json
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ ricoig16k.zip
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ ricosca.json
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ ricosca.zip
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ seeclick_mi.json
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ seeclick.json
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ seeclick.tar.gz.part_00
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ seeclick.tar.gz.part_01
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ seeclick.tar.gz.part_02
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ seeclick.tar.gz.part_03
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ seeclick.tar.gz.part_04
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ seeclick.tar.gz.part_05
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ ui_refexp.json
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ ui_refexp.zip
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ webui350k.json
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ webui350k.zip
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ widget_captioning.json
â”‚Â Â      â”‚Â Â  â””â”€â”€ widget_captioning.zip
â”‚Â Â      â””â”€â”€ aguvis-stage2
â”‚Â Â          â”œâ”€â”€ aitw-l1.json
â”‚Â Â          â”œâ”€â”€ aitw-l2.json
â”‚Â Â          â”œâ”€â”€ aitw-l3.json
â”‚Â Â          â”œâ”€â”€ aitw.zip
â”‚Â Â          â”œâ”€â”€ amex-l1.json
â”‚Â Â          â”œâ”€â”€ amex-l2.json
â”‚Â Â          â”œâ”€â”€ amex-l3.json
â”‚Â Â          â”œâ”€â”€ amex.zip
â”‚Â Â          â”œâ”€â”€ android_control.json
â”‚Â Â          â”œâ”€â”€ android_control.zip
â”‚Â Â          â”œâ”€â”€ coat.json
â”‚Â Â          â”œâ”€â”€ coat.zip
â”‚Â Â          â”œâ”€â”€ gui-odyssey-l1.json
â”‚Â Â          â”œâ”€â”€ gui-odyssey-l2.json
â”‚Â Â          â”œâ”€â”€ gui-odyssey-l3.json
â”‚Â Â          â”œâ”€â”€ gui-odyssey.tar.gz.part_00
â”‚Â Â          â”œâ”€â”€ gui-odyssey.tar.gz.part_01
â”‚Â Â          â”œâ”€â”€ gui-odyssey.tar.gz.part_02
â”‚Â Â          â”œâ”€â”€ gui-odyssey.tar.gz.part_03
â”‚Â Â          â”œâ”€â”€ guiact-web-multi-l1.json
â”‚Â Â          â”œâ”€â”€ guiact-web-multi-l2.json
â”‚Â Â          â”œâ”€â”€ guiact-web-multi-l3.json
â”‚Â Â          â”œâ”€â”€ guiact-web-multi.zip
â”‚Â Â          â”œâ”€â”€ guiact-web-single.json
â”‚Â Â          â”œâ”€â”€ guiact-web-single.zip
â”‚Â Â          â”œâ”€â”€ guide.json
â”‚Â Â          â”œâ”€â”€ guide.zip
â”‚Â Â          â”œâ”€â”€ mind2web-l1.json
â”‚Â Â          â”œâ”€â”€ mind2web-l2.json
â”‚Â Â          â”œâ”€â”€ mind2web-l3.json
â”‚Â Â          â”œâ”€â”€ mind2web.zip
â”‚Â Â          â”œâ”€â”€ miniwob-l1.json
â”‚Â Â          â”œâ”€â”€ miniwob-l2.json
â”‚Â Â          â”œâ”€â”€ miniwob-l3.json
â”‚Â Â          â”œâ”€â”€ miniwob.zip
â”‚Â Â          â””â”€â”€ README.md
â”œâ”€â”€ docker
â”‚Â Â  â”œâ”€â”€ Dockerfile.inference
â”‚Â Â  â””â”€â”€ Dockerfile.train
â”œâ”€â”€ eval
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ eval_dataset.py
â”‚Â Â  â”œâ”€â”€ eval_prompt.py
â”‚Â Â  â””â”€â”€ eval.py
â”œâ”€â”€ notebooks
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ train
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ collator.py
â”‚Â Â  â”œâ”€â”€ config
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ stage1_grounding.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ stage2_reasoning.yaml
â”‚Â Â  â”œâ”€â”€ datasets.py
â”‚Â Â  â””â”€â”€ train.py
â”œâ”€â”€ utils
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ action_conversion.py
â”‚Â Â  â””â”€â”€ function_parser.py
â””â”€â”€ uv.lock

15 directories, 87 files

Backend Tree:
backend
â”œâ”€â”€ main.py
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md

1 directory, 3 files

Frontend Tree (web/):
web
â”œâ”€â”€ eslint.config.mjs
â”œâ”€â”€ next-env.d.ts
â”œâ”€â”€ next.config.ts
â”œâ”€â”€ package-lock.json
â”œâ”€â”€ package.json
â”œâ”€â”€ postcss.config.mjs
â”œâ”€â”€ public
â”‚Â Â  â”œâ”€â”€ file.svg
â”‚Â Â  â”œâ”€â”€ globe.svg
â”‚Â Â  â”œâ”€â”€ next.svg
â”‚Â Â  â”œâ”€â”€ vercel.svg
â”‚Â Â  â””â”€â”€ window.svg
â”œâ”€â”€ README.md
â”œâ”€â”€ src
â”‚Â Â  â””â”€â”€ app
â”‚Â Â      â”œâ”€â”€ favicon.ico
â”‚Â Â      â”œâ”€â”€ globals.css
â”‚Â Â      â”œâ”€â”€ layout.tsx
â”‚Â Â      â””â”€â”€ page.tsx
â”œâ”€â”€ tsconfig.json
â””â”€â”€ tsconfig.tsbuildinfo

4 directories, 18 files

-----------------------

Project Context: The Churninator - Autonomous AI Mystery Shopper for SaaS

Core Concept: An open-source, AI-powered platform designed to autonomously analyze the signup and onboarding experience of any web application. It acts as a "mystery shopper," identifying UX friction points, conversion killers, and usability issues, then presents the findings in a detailed, interactive report.

System Architecture: Multi-Component, Cloud-Native Platform

1. The Forge (forge/):
Role: The "AI Workshop & ML Pipeline."
Function: Contains all scripts for the two-stage AGUVIS fine-tuning methodology (Stage 1: Grounding, Stage 2: Reasoning). It's designed for full-scale training on remote cloud GPUs (like Vast.ai).

2. The Backend (backend/):
Role: The "Mission Control & Production Engine."
Sub-components:
- API / Control Plane (backend/api/): FastAPI server for user requests and job queuing (Redis).
- Agent Worker (backend/worker/): Scalable Celery workers running Playwright in a virtual display (Xvfb).
- Inference Server (backend/inference/): Hosts the fine-tuned Churninator model (e.g., using vLLM), acting as the remote brain for the workers.

3. The Frontend (web/):
Role: The "Observatory & Executive Dashboard."
Function: A Next.js application with a "live view" dashboard (WebSockets for logs, MJPEG for video) to observe agents in real-time.

User Experience & Core Loop:
A user submits a URL and task. The API queues a job. A Worker picks it up, streams its browser view and "thoughts" to the frontend, and generates a final report on all discovered UX issues.
