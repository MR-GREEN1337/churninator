File: forge/__init__.py
-e
-e
-e
File: forge/utils/function_parser.py
# forge/utils/function_parser.py
# (This is the complete, correct code from the smoloperator context)
import re
from typing import Dict, List, Tuple, Any
from collections import OrderedDict
from pydantic import BaseModel


class FunctionCall(BaseModel):
    function_name: str
    parameters: Dict[str, Any]
    original_string: str
    description: str = ""

    def to_string(self) -> str:
        if not self.parameters:
            return f"{self.function_name}()"
        positional_args, named_args = [], []
        for name, value in self.parameters.items():
            if name.startswith("arg_"):
                positional_args.append((int(name.split("_")[1]), value))
            else:
                named_args.append((name, value))
        positional_args.sort(key=lambda x: x[0])
        param_parts = [self._value_to_string(v) for _, v in positional_args]
        param_parts.extend(f"{n}={self._value_to_string(v)}" for n, v in named_args)
        return f"{self.function_name}({', '.join(param_parts)})"

    def _value_to_string(self, value: Any) -> str:
        if isinstance(value, str):
            return f"'{value}'"
        if isinstance(value, (list, tuple)):
            return f"[{', '.join(self._value_to_string(i) for i in value)}]"
        if isinstance(value, dict):
            return f"{{{', '.join(f'{self._value_to_string(k)}: {self._value_to_string(v)}' for k, v in value.items())}}}"
        if isinstance(value, bool):
            return str(value).lower()
        return str(value)


def parse_function_call(s: str) -> List[FunctionCall]:
    pattern = r"([a-zA-Z_][a-zA-Z0-9_.]*)\(([^)]*)\)"
    matches = re.findall(pattern, s.strip())
    results = []
    for match in matches:
        name, params_str = match[0], match[1]
        params = parse_parameters(params_str)
        results.append(
            FunctionCall(
                function_name=name,
                parameters=params,
                original_string=f"{name}({params_str})",
            )
        )
    return results


def parse_parameters(params_str: str) -> Dict[str, Any]:
    if not params_str.strip():
        return {}
    params = OrderedDict()
    parts = split_parameters(params_str)
    pos_idx = 0
    for part in parts:
        part = part.strip()
        if not part:
            continue
        name, value = parse_single_parameter(part)
        if name.startswith("arg_"):
            name = f"arg_{pos_idx}"
            pos_idx += 1
        params[name] = value
    return params


def split_parameters(params_str: str) -> List[str]:
    parts, current_part, depth, in_quotes = [], "", 0, False
    for char in params_str:
        if char in "\"'":
            in_quotes = not in_quotes
        elif not in_quotes and char == "(":
            depth += 1
        elif not in_quotes and char == ")":
            depth -= 1
        elif not in_quotes and char == "," and depth == 0:
            parts.append(current_part.strip())
            current_part = ""
            continue
        current_part += char
    if current_part.strip():
        parts.append(current_part.strip())
    return parts


def parse_single_parameter(param_str: str) -> Tuple[str, Any]:
    match = re.match(r"^([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*(.+)$", param_str)
    if match:
        return match.group(1), parse_value(match.group(2).strip())
    return "arg_0", parse_value(param_str)


def parse_value(v_str: str) -> Any:
    v_str = v_str.strip()
    if (v_str.startswith("'") and v_str.endswith("'")) or (
        v_str.startswith('"') and v_str.endswith('"')
    ):
        return v_str[1:-1]
    if v_str.lower() == "true":
        return True
    if v_str.lower() == "false":
        return False
    try:
        if "." in v_str:
            return float(v_str)
        return int(v_str)
    except ValueError:
        return v_str


def extract_function_calls_from_text(text: str) -> List[FunctionCall]:
    pattern = r"[a-zA-Z_][a-zA-Z0-9_.]*\([^)]*\)"
    matches = re.findall(pattern, text)
    results = []
    for m in matches:
        results.extend(parse_function_call(m))
    return results
-e
-e
-e
File: forge/utils/__init__.py
-e
-e
-e
File: forge/utils/action_conversion.py
# forge/data/processing/action_conversion.py
# (This is the complete, correct code from the smoloperator context)
from forge.utils.function_parser import FunctionCall


def rename_parameters(action: FunctionCall):
    if not action.parameters:
        return
    new_params = {f"arg_{i}": v for i, v in enumerate(action.parameters.values())}
    action.parameters = new_params


def change_argument_name(action: FunctionCall):
    if "arg_0" in action.parameters:
        if isinstance(action.parameters["arg_0"], (list, tuple)):
            action.parameters["from_coord"] = tuple(
                float(c) for c in action.parameters["arg_0"]
            )
        else:
            action.parameters["x"] = float(action.parameters["arg_0"])
        del action.parameters["arg_0"]
    if "arg_1" in action.parameters:
        if isinstance(action.parameters["arg_1"], (list, tuple)):
            action.parameters["to_coord"] = tuple(
                float(c) for c in action.parameters["arg_1"]
            )
        else:
            action.parameters["y"] = float(action.parameters["arg_1"])
        del action.parameters["arg_1"]


def action_conversion(
    actions: list[FunctionCall], resolution: tuple[int, int]
) -> list[FunctionCall]:
    for i, action in enumerate(actions):
        rename_parameters(action)
        name = action.function_name
        if name == "mobile.home":
            action.function_name = "navigate_home"
        elif name == "mobile.open_app":
            action.function_name = "open_app"
        elif name == "mobile.swipe":
            action.function_name = "swipe"
            change_argument_name(action)
        elif name == "mobile.back":
            action.function_name = "navigate_back"
        elif name == "mobile.long_press":
            action.function_name = "long_press"
            change_argument_name(action)
        elif name in ["mobile.terminate", "answer"]:
            action.function_name = "final_answer"
        elif name == "mobile.wait":
            action.function_name = "wait"
            if "arg_0" in action.parameters:
                action.parameters["seconds"] = int(action.parameters.pop("arg_0"))
        elif name == "pyautogui.click":
            action.function_name = "click"
            change_argument_name(action)
        elif name == "pyautogui.doubleClick":
            action.function_name = "double_click"
            change_argument_name(action)
        elif name == "pyautogui.rightClick":
            action.function_name = "right_click"
            change_argument_name(action)
        elif name in ["pyautogui.hotkey", "pyautogui.press"]:
            action.function_name = "press"
            if "arg_0" in action.parameters:
                action.parameters["keys"] = action.parameters.pop("arg_0")
        elif name == "pyautogui.moveTo":
            action.function_name = "move_mouse"
            change_argument_name(action)
        elif name == "pyautogui.write":
            action.function_name = "type"
            if "arg_0" in action.parameters:
                action.parameters["text"] = action.parameters.pop("arg_0")
        elif name in ["pyautogui.scroll", "pyautogui.hscroll"]:
            val = action.parameters.pop("arg_0")
            action.function_name = "scroll"
            action.parameters["amount"] = int(abs(val * 100))
            if name == "pyautogui.hscroll":
                action.parameters["direction"] = "left" if val < 0 else "right"
            else:
                action.parameters["direction"] = "up" if val < 0 else "down"
        elif name == "pyautogui.dragTo":
            action.function_name = "drag"
            change_argument_name(action)
        action.original_string = action.to_string()
    return actions
-e
-e
-e
File: forge/train/collator.py
# forge/training/collator.py
from typing import List, Dict, Any
import torch
from transformers import AutoProcessor


class FinetuneDataCollator:
    """
    A specialized data collator for the Churninator fine-tuning process.

    This collator takes a batch of data items from the FinetuneDataset and
    formats them into the precise structure expected by modern Vision-Language
    Models for training. It handles dynamic batching of images and text,
    padding, tokenization, and the crucial task of masking labels.
    """

    def __init__(self, processor: AutoProcessor, max_length: int = 4096):
        """
        Initializes the collator.

        Args:
            processor: The Hugging Face AutoProcessor for the model (e.g., for SmolLM/Idefics2).
            max_length (int): The maximum sequence length for the model input.
        """
        self.processor = processor
        self.max_length = max_length

    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """
        The main collating function that transforms a list of dataset items into a batch.

        Args:
            examples (List[Dict[str, Any]]): A list of dictionaries from our
                FinetuneDataset. Each item has 'image' (a PIL Image) and 'messages'
                (a list of conversation turn dictionaries).

        Returns:
            Dict[str, torch.Tensor]: A dictionary of batched and padded tensors
                ready for the model, including 'input_ids', 'attention_mask',
                pixel_values, and 'labels'.
        """

        # Extract images and message lists from the batch
        images = [example["image"] for example in examples]
        batch_messages = [example["messages"] for example in examples]

        # Use the processor's chat template to convert the structured message lists
        # into a single, correctly formatted prompt string for each example.
        # This handles all the special tokens (e.g., <|user|>, <|assistant|>) automatically.
        full_texts = [
            self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=False
            )
            for messages in batch_messages
        ]

        # Tokenize the text and process the images all at once.
        # This is the most efficient method and correctly handles multimodal inputs.
        batch = self.processor(
            text=full_texts,
            images=images,
            padding="longest",  # Pad to the longest sequence in the batch
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt",
        )

        # --- LABEL MASKING ---
        # Our goal is to train the model *only* on the assistant's responses.
        # We do this by creating a 'labels' tensor where all tokens belonging to
        # the user or system prompts are masked with -100.

        labels = batch["input_ids"].clone()

        # To find where the assistant's part begins, we can process the prompts
        # again, but this time *without* the assistant's final response.
        prompts_without_assistant = []
        for messages in batch_messages:
            # Create a version of the conversation with the last (assistant) message removed
            prompt_only_messages = messages[:-1]
            prompt_text = self.processor.apply_chat_template(
                prompt_only_messages, tokenize=False, add_generation_prompt=False
            )
            prompts_without_assistant.append(prompt_text)

        # Tokenize these "prompt-only" texts to find their length
        prompt_lengths = self.processor(
            text=prompts_without_assistant,
            padding=False,  # We only need the length, not padding
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt",
        )["input_ids"].shape[1]

        # For each item in the batch, mask the prompt part
        for i in range(len(examples)):
            prompt_len = prompt_lengths[i]
            # Set all tokens up to the end of the prompt to -100
            labels[i, :prompt_len] = -100
            # Also mask any padding tokens at the end
            # The attention mask is 1 for real tokens and 0 for padding.
            labels[i][batch["attention_mask"][i] == 0] = -100

        batch["labels"] = labels
        return batch
-e
-e
-e
File: forge/train/datasets.py
# forge/train/datasets.py
import json
from pathlib import Path
from torch.utils.data import Dataset
from PIL import Image


class FinetuneDataset(Dataset):
    """
    Loads and processes data for both Stage 1 (Grounding) and Stage 2 (Reasoning)
    of the Churninator fine-tuning process.
    """

    def __init__(self, jsonl_file: str, processor, raw_data_root: str, stage: int):
        """
        Initializes the dataset.

        Args:
            jsonl_file (str): Path to the processed .jsonl data file.
            processor: The Hugging Face model processor.
            raw_data_root (str): Path to the root of the raw data directory (for resolving image paths).
            stage (int): The training stage (1 or 2) to configure the prompt format.
        """
        self.data = [json.loads(line) for line in open(jsonl_file)]
        self.processor = processor
        self.raw_data_root = Path(raw_data_root)
        self.stage = stage

        if self.stage not in [1, 2]:
            raise ValueError(f"Invalid stage specified: {self.stage}. Must be 1 or 2.")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx: int):
        item = self.data[idx]

        # Resolve the full image path
        image_path = self.raw_data_root / item["image_path"]
        try:
            image = Image.open(image_path).convert("RGB")
        except Exception as e:
            print(
                f"Warning: Could not load image at {image_path}. Error: {e}. Skipping."
            )
            # Recursively try the next item if the current one is broken
            return self.__getitem__((idx + 1) % len(self))

        # --- DYNAMIC PROMPT FORMATTING based on stage ---
        if self.stage == 1:
            # Stage 1: Simple Instruction -> Action for Grounding
            messages = [
                {
                    "role": "system",
                    "content": "You are a helpful GUI agent. You will be given an instruction and a screenshot. Respond with the single pyautogui-style action to complete the instruction.",
                },
                {"role": "user", "content": f"Instruction: {item['instruction']}"},
                {"role": "assistant", "content": f"Action: {item['action']}"},
            ]

        elif self.stage == 2:
            # Stage 2: Instruction -> Thought & Action for Reasoning
            # This format teaches the model the "inner monologue" structure.
            # Assumes the preprocessed data for stage 2 has 'thought' and 'action' keys.
            thought = item.get(
                "thought", "I need to analyze the screen and decide the next step."
            )  # Default thought
            action = item.get("action", "TERMINATE('missing action')")

            assistant_response = f"<think>{thought}</think>\n<code>{action}</code>"
            messages = [
                {
                    "role": "system",
                    "content": "You are a helpful GUI agent. First, think step-by-step about your plan inside <think> tags. Then, provide the single action to perform inside <code> tags.",
                },
                {"role": "user", "content": f"Instruction: {item['instruction']}"},
                {"role": "assistant", "content": assistant_response},
            ]

        # Return the raw image and messages; the collator will handle tokenization.
        return {"image": image, "messages": messages}
-e
-e
-e
File: forge/train/__init__.py
-e
-e
-e
File: forge/train/train.py
# forge/train/train.py
import yaml
import os
from pathlib import Path
import torch
import wandb
from transformers import (
    AutoProcessor,
    Idefics2ForConditionalGeneration,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, PeftModel
import argparse

# --- Local Project Imports ---
from forge.train.datasets import FinetuneDataset
from forge.train.collator import FinetuneDataCollator


def main(config_path: str):
    """
    The main training function for the Churninator Forge.
    Orchestrates the entire fine-tuning process based on hierarchical YAML configs.
    """
    # --- 1. Load Hierarchical Configurations ---
    print("--- Loading Configurations ---")
    with open("forge/config.yaml", "r") as f:
        master_config = yaml.safe_load(f)

    with open(config_path, "r") as f:
        run_config = yaml.safe_load(f)

    # Merge configs: run_config overrides defaults in master_config['defaults']
    config = {**master_config.get("defaults", {}), **run_config}

    # Add non-default, essential keys from master_config
    for key in [
        "hf_token",
        "wandb_project",
        "wandb_entity",
        "output_dir",
        "base_model",
    ]:
        if key in master_config and key not in config:
            config[key] = master_config[key]

    # Crucially, allow the run_config to override the master `base_model` for Stage 2
    if "base_model" in run_config:
        config["base_model"] = run_config["base_model"]

    print("--- Effective Run Configuration ---")
    print(yaml.dump(config))
    print("---------------------------------")

    # --- 2. Initialize W&B ---
    wandb.init(
        project=config["wandb_project"],
        entity=config["wandb_entity"],
        name=config["output_dir"],
        config=config,
        resume="allow",  # Allows resuming a previous run
    )

    # --- 3. Model & Processor Loading ---
    print(f"Loading base model config from: {config['base_model']}")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    )

    # The `base_model_id` is always the original model from the Hub.
    # For Stage 2, we load this first, then apply our Stage 1 adapters.
    base_model_id = master_config["base_model"]
    processor = AutoProcessor.from_pretrained(base_model_id, token=config["hf_token"])

    model = Idefics2ForConditionalGeneration.from_pretrained(
        base_model_id,
        quantization_config=quantization_config,
        device_map="auto",
        token=config["hf_token"],
    )

    # --- 4. DYNAMIC MODEL SETUP (Stage 1 vs Stage 2) ---
    is_stage_2 = os.path.isdir(config["base_model"])
    if is_stage_2:
        print("--- STAGE 2 RUN DETECTED ---")
        print(f"Loading Stage 1 LoRA adapters from: {config['base_model']}")
        model = PeftModel.from_pretrained(model, config["base_model"])
        print("Successfully loaded Stage 1 adapters.")
    else:
        print("--- STAGE 1 RUN DETECTED ---")

    # --- 5. LoRA Configuration ---
    lora_config = LoraConfig(
        r=config["lora_r"],
        lora_alpha=config["lora_alpha"],
        lora_dropout=config["lora_dropout"],
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],
        use_rslora=True,
        bias="none",
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # --- 6. Dataset Loading ---
    stage_num = 2 if "reasoning" in config["data_file"] else 1
    print(f"Configuring dataset for training Stage {stage_num}.")

    data_file = Path(master_config["data_dir"]) / "processed" / config["data_file"]
    raw_data_root = Path(master_config["data_dir"]) / "raw"

    train_dataset = FinetuneDataset(
        str(data_file), processor, str(raw_data_root), stage=stage_num
    )
    data_collator = FinetuneDataCollator(processor)

    # --- 7. Training Arguments ---
    output_dir_final = Path(master_config["output_dir"]) / config["output_dir"]
    training_args = TrainingArguments(
        output_dir=str(output_dir_final),
        num_train_epochs=config["num_train_epochs"],
        per_device_train_batch_size=config["per_device_train_batch_size"],
        gradient_accumulation_steps=config["gradient_accumulation_steps"],
        gradient_checkpointing=config["gradient_checkpointing"],
        learning_rate=config["learning_rate"],
        optim=config["optim"],
        logging_steps=config["logging_steps"],
        save_strategy=config["save_strategy"],
        save_steps=config.get("save_steps", 500),  # Use get for optional keys
        save_total_limit=config.get("save_total_limit", 2),
        report_to=config["report_to"],
        bf16=config.get("bf16", False),
        fp16=config.get("fp16", False),
        remove_unused_columns=False,
    )

    # --- 8. Initialize Trainer ---
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )

    # --- 9. START or RESUME TRAINING ---
    print("🔥🔥🔥 The Forge is lit. Fine-tuning will now begin or resume. 🔥🔥🔥")

    # The Trainer will automatically look for the latest checkpoint in `output_dir`.
    # Passing `True` is the most explicit way to enable this behavior.
    trainer.train(resume_from_checkpoint=True)

    print("✅ Training complete!")

    # --- 10. Save Final Model ---
    final_path = output_dir_final / "final"
    trainer.save_model(str(final_path))
    print(f"✅ Final model adapters saved to {final_path}")
    wandb.finish()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="The Churninator Forge Training Script"
    )
    parser.add_argument(
        "--config",
        required=True,
        help="Path to the RUN-SPECIFIC config YAML (e.g., forge/train/configs/stage1_grounding.yaml).",
    )
    args = parser.parse_args()
    main(args.config)
-e
-e
-e
File: forge/eval/__init__.py
-e
-e
-e
File: forge/eval/eval_prompt.py
# forge/eval/eval_prompt.py

# --- AGUVIS Stage 1 (Grounding) Evaluation Prompt ---
# Tests the model's basic ability to map an instruction directly to an action.
# EXPECTS OUTPUT: "Action: click(x=..., y=...)"

SCREENSPOT_V2_USER_PROMPT_PHASE_1 = """Using the screenshot, you will get an instruction and will need to output the action that completes the instruction or targets the given element.

Just write your action as follows:

Action: click(x=0.XXXX, y=0.YYYY)
With 0.XXXX and 0.YYYY the normalized coordinates of the click position on the screenshot.

Now write the action needed to complete the instruction:
Instruction: {instruction}
"""


# --- AGUVIS Stage 2 (Reasoning) Evaluation Prompt ---
# Tests the model's ability to use the "Thought -> Action" process.
# EXPECTS OUTPUT: "<think>...</think>\n<code>click(x=..., y=...)</code>"

# The system prompt defines the rules and the tools available.
STAGE_2_SYSTEM_PROMPT = """You are a helpful GUI agent. Your goal is to complete the given instruction. First, think step-by-step about your plan inside <think> tags. Then, provide the single action to perform inside <code> tags."""

# The user prompt provides the specific task.
STAGE_2_USER_PROMPT = """Instruction: {instruction}"""
-e
-e
-e
File: forge/eval/eval_dataset.py
# forge/eval/eval_dataset.py
import json
from pathlib import Path
from torch.utils.data import Dataset
from PIL import Image


class ScreenSpotDataset(Dataset):
    """Loads the ScreenSpot benchmark dataset for quantitative evaluation."""

    def __init__(self, data_root: str, split: str = "all"):
        self.data_root = Path(data_root)
        self.split = split
        self.data = self._load_data()

    def _load_data(self):
        all_data = []
        # ScreenSpot has web, mobile, desktop splits
        splits_to_load = (
            ["web", "mobile", "desktop"] if self.split == "all" else [self.split]
        )
        for s in splits_to_load:
            json_path = self.data_root / f"annotations/{s}.json"
            with open(json_path, "r") as f:
                data = json.load(f)
                for item in data:
                    item["split"] = s
                    all_data.append(item)
        return all_data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        image_path = self.data_root / f"screenshot/{item['split']}/{item['image']}"
        try:
            image = Image.open(image_path).convert("RGB")
        except Exception:
            # Fallback for corrupted data
            return self.__getitem__((idx + 1) % len(self))

        # Ground truth bounding box (left, top, right, bottom)
        bbox = [
            item["bbox"][0],
            item["bbox"][1],
            item["bbox"][0] + item["bbox"][2],
            item["bbox"][1] + item["bbox"][3],
        ]

        return {
            "image": image,
            "instruction": item["instruction"],
            "ground_truth_bbox": bbox,
        }
-e
-e
-e
File: forge/eval/eval.py
# forge/eval/eval.py
import argparse
import torch
from transformers import AutoProcessor, Idefics2ForConditionalGeneration
from peft import PeftModel
from PIL import Image
import requests
from pathlib import Path
from tqdm import tqdm
import sys
import re

# Add project root to path for imports
sys.path.append(str(Path(__file__).resolve().parents[2]))

from forge.eval.eval_prompt import (
    SCREENSPOT_V2_USER_PROMPT_PHASE_1,
    STAGE_2_SYSTEM_PROMPT,
    STAGE_2_USER_PROMPT,
)
from forge.utils.function_parser import extract_function_calls_from_text
from forge.eval.eval_dataset import ScreenSpotDataset


def load_model_for_eval(model_path: str):
    """Loads a fine-tuned PEFT model and merges it for fast inference."""
    print(f"--- Loading model from: {model_path} ---")
    base_model_id = "HuggingFaceTB/SmolLM-1.7B-32k-instruct"

    dtype = torch.float16 if torch.backends.mps.is_available() else torch.bfloat16

    # Load the base model first, in 4-bit for memory efficiency on the host
    # When we merge the adapter, it will be upcasted.
    model = Idefics2ForConditionalGeneration.from_pretrained(
        base_model_id, torch_dtype=dtype
    )

    # Apply the LoRA adapters from your fine-tuning run
    model = PeftModel.from_pretrained(model, model_path)
    model = model.merge_and_unload()  # Merge adapters for fast inference

    device = (
        "mps"
        if torch.backends.mps.is_available()
        else "cuda"
        if torch.cuda.is_available()
        else "cpu"
    )
    model.to(device)

    processor = AutoProcessor.from_pretrained(base_model_id)
    print(f"✅ Model loaded to {device}.")
    return model, processor


def parse_action_from_stage2_output(text: str) -> list:
    """Extracts the action from the <code> block of a Stage 2 model output."""
    code_match = re.search(r"<code>(.*?)</code>", text, re.DOTALL)
    if code_match:
        code_content = code_match.group(1).strip()
        return extract_function_calls_from_text(code_content)
    return []


def run_evaluation(model, processor, stage, eval_type, **kwargs):
    """Unified evaluation runner."""
    if eval_type == "qualitative":
        run_qualitative_test(model, processor, stage, **kwargs)
    elif eval_type == "quantitative":
        run_quantitative_benchmark(model, processor, stage, **kwargs)


def run_qualitative_test(model, processor, stage, image_url, instruction):
    """Runs a single, qualitative spot-check on the model."""
    print("\n--- Running Qualitative Spot-Check ---")
    print(f"Fetching image from: {image_url}")
    image = Image.open(requests.get(image_url, stream=True).raw).convert("RGB")

    # --- DYNAMIC PROMPT SELECTION ---
    if stage == 1:
        prompt_text = SCREENSPOT_V2_USER_PROMPT_PHASE_1.format(instruction=instruction)
        messages = [
            {
                "role": "user",
                "content": [{"type": "image"}, {"type": "text", "text": prompt_text}],
            }
        ]
    else:  # Stage 2
        messages = [
            {"role": "system", "content": STAGE_2_SYSTEM_PROMPT},
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {
                        "type": "text",
                        "text": STAGE_2_USER_PROMPT.format(instruction=instruction),
                    },
                ],
            },
        ]

    inputs = processor(messages, image=image, return_tensors="pt").to(model.device)

    print("🤖 Generating prediction...")
    generated_ids = model.generate(**inputs, max_new_tokens=100)
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    print("\n--- RESULTS ---")
    print(f"Instruction: {instruction}")
    print(f"Full Model Output:\n{generated_text}")

    # --- DYNAMIC PARSING ---
    parsed_actions = (
        parse_action_from_stage2_output(generated_text)
        if stage == 2
        else extract_function_calls_from_text(generated_text)
    )

    if parsed_actions:
        print(f"\n✅ Parsed Action: {parsed_actions[0].to_string()}")
    else:
        print("\n⚠️ Could not parse an action from the model's output.")


def check_click_accuracy(pred_x, pred_y, gt_bbox_norm):
    """Checks if a predicted click (normalized) is within a ground truth bbox (normalized)."""
    return (
        gt_bbox_norm[0] <= pred_x <= gt_bbox_norm[2]
        and gt_bbox_norm[1] <= pred_y <= gt_bbox_norm[3]
    )


def run_quantitative_benchmark(model, processor, stage, data_root, limit):
    """Runs a quantitative benchmark on the ScreenSpot dataset."""
    print(f"\n--- Running Quantitative Benchmark (Stage {stage}) on ScreenSpot ---")
    dataset = ScreenSpotDataset(data_root=data_root)
    dataset_subset = torch.utils.data.Subset(dataset, range(min(limit, len(dataset))))

    correct_predictions = 0
    for item in tqdm(dataset_subset, desc="Benchmarking"):
        image, instruction, gt_bbox_pixels = (
            item["image"],
            item["instruction"],
            item["ground_truth_bbox"],
        )
        img_width, img_height = image.size
        gt_bbox_norm = [
            gt_bbox_pixels[0] / img_width,
            gt_bbox_pixels[1] / img_height,
            gt_bbox_pixels[2] / img_width,
            gt_bbox_pixels[3] / img_height,
        ]

        if stage == 1:
            prompt_text = SCREENSPOT_V2_USER_PROMPT_PHASE_1.format(
                instruction=instruction
            )
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": prompt_text},
                    ],
                }
            ]
        else:  # Stage 2
            messages = [
                {"role": "system", "content": STAGE_2_SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {
                            "type": "text",
                            "text": STAGE_2_USER_PROMPT.format(instruction=instruction),
                        },
                    ],
                },
            ]

        inputs = processor(messages, image=image, return_tensors="pt").to(model.device)
        generated_ids = model.generate(**inputs, max_new_tokens=100)
        generated_text = processor.batch_decode(
            generated_ids, skip_special_tokens=True
        )[0]

        parsed_actions = (
            parse_action_from_stage2_output(generated_text)
            if stage == 2
            else extract_function_calls_from_text(generated_text)
        )

        if parsed_actions and parsed_actions[0].function_name == "click":
            params = parsed_actions[0].parameters
            if "x" in params and "y" in params:
                if check_click_accuracy(params["x"], params["y"], gt_bbox_norm):
                    correct_predictions += 1

    accuracy = (correct_predictions / limit) * 100
    print("\n--- BENCHMARK COMPLETE ---")
    print(f"Total Samples: {limit}")
    print(f"Correct Predictions: {correct_predictions}")
    print(f"Accuracy: {accuracy:.2f}%")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Churninator Model Evaluation Suite")
    parser.add_argument(
        "--model-path",
        required=True,
        help="Path to the fine-tuned PEFT model checkpoint directory.",
    )
    parser.add_argument(
        "--stage",
        required=True,
        type=int,
        choices=[1, 2],
        help="Which fine-tuning stage is this model from (1 or 2).",
    )
    parser.add_argument(
        "--eval-type",
        required=True,
        choices=["qualitative", "quantitative"],
        help="Type of evaluation to run.",
    )
    parser.add_argument("--image-url", help="URL of the image for qualitative test.")
    parser.add_argument("--instruction", help="Instruction for qualitative test.")
    parser.add_argument(
        "--data-root",
        default="forge/data/eval_data/ScreenSpot_v2",
        help="Root directory of the ScreenSpot evaluation data.",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=100,
        help="Number of samples for quantitative benchmark.",
    )

    args = parser.parse_args()

    model, processor = load_model_for_eval(args.model_path)

    kwargs = {
        "image_url": args.image_url,
        "instruction": args.instruction,
        "data_root": args.data_root,
        "limit": args.limit,
    }

    run_evaluation(model, processor, args.stage, args.eval_type, **kwargs)
-e
-e
-e
File: forge/data/preprocess.py
# forge/data/preprocess.py
import json
from pathlib import Path
from datasets import load_dataset
from tqdm import tqdm
import logging

from forge.utils.function_parser import parse_function_call
from forge.utils.action_conversion import action_conversion

# Setup basic logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def process_stage(stage_name, stage_num, raw_data_dir, output_dir):
    """Processes a single stage of the AGUVIS dataset, now with action conversion."""
    raw_stage_path = raw_data_dir / f"aguvis-stage{stage_num}"
    if not raw_stage_path.exists():
        logging.error(f"Raw data path not found: {raw_stage_path}")
        return

    logging.info(f"--- Starting processing for {stage_name} ---")

    dataset = load_dataset(str(raw_stage_path), name="default", split="train")
    output_file = output_dir / f"stage{stage_num}_{stage_name.lower()}.jsonl"
    logging.info(f"Processing and saving to {output_file}...")

    count = 0
    conversion_errors = 0
    with open(output_file, "w") as f:
        for item in tqdm(dataset, desc=f"Processing {stage_name}"):
            try:
                # --- ACTION CONVERSION LOGIC ---
                # 1. Parse the raw action string using our universal translator
                original_action_str = item["action"]
                parsed_calls = parse_function_call(original_action_str)

                if not parsed_calls:
                    continue  # Skip if the action is malformed

                # 2. Convert to our clean, unified action space using the AGUVIS specialist
                # We assume a dummy resolution for now, as the model will learn normalized coords.
                # The actual pixel values don't matter during this conversion step.
                converted_calls = action_conversion(parsed_calls, resolution=(1, 1))

                # 3. Reconstruct the clean action string
                clean_action_str = " ".join(
                    [call.to_string() for call in converted_calls]
                )
                # --- END ACTION CONVERSION ---

                portable_image_path = str(
                    Path(f"aguvis-stage{stage_num}") / item["image"]
                )

                processed_item = {
                    "image_path": portable_image_path,
                    "instruction": item["instruction"],
                    "action": clean_action_str,  # Use the clean, converted action
                }
                f.write(json.dumps(processed_item) + "\n")
                count += 1
            except Exception as e:
                conversion_errors += 1
                logging.warning(
                    f"Skipping record due to conversion error: {e} on action '{item.get('action', 'N/A')}'"
                )

    logging.info(
        f"✅ {stage_name} processing complete. Wrote {count} records to {output_file}"
    )
    if conversion_errors > 0:
        logging.warning(f"⚠️ Encountered {conversion_errors} action conversion errors.")


def main():
    raw_data_dir = Path("forge/data/raw")
    output_dir = Path("forge/data/processed")
    output_dir.mkdir(parents=True, exist_ok=True)

    process_stage("grounding", 1, raw_data_dir, output_dir)
    # You can uncomment this later when you're ready for Stage 2
    # process_stage("reasoning", 2, raw_data_dir, output_dir)


if __name__ == "__main__":
    main()
-e
-e
-e
File: forge/data/processing/__init__.py
-e
-e
-e
File: forge/data/__init__.py
-e
-e
-e
File: backend/src/core/security.py
# backend/src/core/security.py
from datetime import datetime, timedelta, timezone
from passlib.context import CryptContext
from jose import jwt
from .settings import get_settings

settings = get_settings()

# Hashing passwords
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def verify_password(plain_password: str, hashed_password: str) -> bool:
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    return pwd_context.hash(password)

# JWT Token Creation
def create_access_token(data: dict) -> str:
    to_encode = data.copy()
    expire = datetime.now(timezone.utc) + timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, settings.SECRET_KEY, algorithm=settings.ALGORITHM)
    return encoded_jwt-e
-e
-e
File: backend/src/core/__init__.py
-e
-e
-e
File: backend/src/core/settings.py
# backend/src/core/settings.py
import os
from functools import lru_cache
from pydantic import PostgresDsn, RedisDsn
from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Literal

class Settings(BaseSettings):
    # Load settings from .env, but allow .env.test to override if ENVIRONMENT=test
    model_config = SettingsConfigDict(
        env_file=".env.test" if os.getenv("ENVIRONMENT") == "test" else ".env",
        env_file_encoding='utf-8',
        extra='ignore'
    )

    # --- Application ---
    ENVIRONMENT: str = "development"
    PROJECT_NAME: str = "The Churninator"
    DEBUG: bool = True
    API_V1_STR: str = "/api/v1"
    INFERENCE_SERVER_URL: str = "http://inference:8001/predict"

    # --- Security & JWT ---
    # Generate a good secret key with: openssl rand -hex 32
    SECRET_KEY: str
    ALGORITHM: str = "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 # 24 hours
    REFRESH_TOKEN_EXPIRE_DAYS: int = 30

    # --- Database (PostgreSQL) ---
    POSTGRES_USER: str
    POSTGRES_PASSWORD: str
    POSTGRES_SERVER: str
    POSTGRES_DB: str
    DB_SSL_MODE: Literal["disable", "require", "verify-ca", "verify-full"] = "disable"
    DB_ECHO_LOG: bool = False

    # Connection Pooling
    POSTGRES_POOL_SIZE: int = 10
    POSTGRES_MAX_OVERFLOW: int = 5

    @property
    def DATABASE_URL(self) -> PostgresDsn:
        return f"postgresql+asyncpg://{self.POSTGRES_USER}:{self.POSTGRES_PASSWORD}@{self.POSTGRES_SERVER}/{self.POSTGRES_DB}"

    # --- Redis / Dramatiq ---
    REDIS_HOST: str
    REDIS_PORT: int = 6379

    @property
    def REDIS_URL(self) -> RedisDsn:
        return f"redis://{self.REDIS_HOST}:{self.REDIS_PORT}"

    # --- CORS ---
    CORS_ORIGINS: list[str] = ["http://localhost:3000"] # For local Next.js dev

@lru_cache
def get_settings() -> Settings:
    """Returns a cached instance of the settings."""
    return Settings()-e
-e
-e
File: backend/src/__init__.py
-e
-e
-e
File: backend/src/utils/__init__.py
-e
-e
-e
File: backend/src/db/postgresql.py
# backend/src/db/postgresql.py
from typing import AsyncGenerator
from sqlmodel.ext.asyncio.session import AsyncSession
from sqlalchemy.ext.asyncio import create_async_engine
from sqlalchemy.orm import sessionmaker
from src.core.settings import get_settings
import ssl
from functools import lru_cache

settings = get_settings()

# --- SSL Configuration ---
connect_args = {}
if settings.DB_SSL_MODE != "disable":
    ssl_context = ssl.create_default_context()
    # In prod, you'd configure this more strictly, e.g., with ca_certs
    connect_args["ssl"] = ssl_context
else:
    connect_args["ssl"] = "disable"

# --- SQLAlchemy Async Engine with Connection Pooling ---
engine = create_async_engine(
    url=str(settings.DATABASE_URL),
    echo=settings.DB_ECHO_LOG,
    pool_size=settings.POSTGRES_POOL_SIZE,
    max_overflow=settings.POSTGRES_MAX_OVERFLOW,
    pool_pre_ping=True,
    connect_args=connect_args,
)

AsyncSessionLocal = sessionmaker(
    engine, class_=AsyncSession, expire_on_commit=False
)

@lru_cache
async def get_db_session() -> AsyncGenerator[AsyncSession, None]:
    """FastAPI dependency to provide a database session per request."""
    async with AsyncSessionLocal() as session:
        yield session-e
-e
-e
File: backend/src/db/__init__.py
-e
-e
-e
File: backend/src/db/models/user.py
# backend/src/db/models/user.py
import uuid
from typing import Optional
from sqlmodel import Field, SQLModel
import datetime as dt

class UserBase(SQLModel):
    email: str = Field(unique=True, index=True)
    is_active: bool = Field(default=True)

class User(UserBase, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True)
    hashed_password: str
    created_at: dt.datetime = Field(default_factory=dt.datetime.utcnow)

class UserCreate(UserBase):
    password: str

class UserRead(UserBase):
    id: uuid.UUID-e
-e
-e
File: backend/src/db/models/agent_run.py
# backend/src/db/models/agent_run.py
import uuid
from typing import Optional, Dict, Any, List
from sqlmodel import Field, SQLModel, Column, Relationship
from sqlalchemy.dialects.postgresql import JSONB
import datetime as dt

class AgentRunBase(SQLModel):
    target_url: str
    task_prompt: str

class AgentRun(AgentRunBase, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True)
    status: str = Field(default="PENDING", index=True)

    # Store the step-by-step log of the agent's run
    run_log: Optional[List[Dict[str, Any]]] = Field(default=None, sa_column=Column(JSONB))

    # Final result/summary from the agent
    final_result: Optional[Dict[str, Any]] = Field(default=None, sa_column=Column(JSONB))

    created_at: dt.datetime = Field(default_factory=dt.datetime.utcnow)

    # Add a foreign key to the user who initiated the run
    owner_id: uuid.UUID = Field(foreign_key="user.id")
    owner: "User" = Relationship(back_populates="runs") # This will require a back-relationship on the User model

class AgentRunCreate(AgentRunBase):
    pass

class AgentRunRead(AgentRunBase):
    id: uuid.UUID
    status: str
    created_at: dt.datetime-e
-e
-e
File: backend/src/db/models/__init__.py
-e
-e
-e
File: backend/src/db/models/report.py
# backend/src/models/report.py
import uuid
from typing import Optional, Dict, Any
from sqlmodel import Field, SQLModel, Column
from sqlalchemy.dialects.postgresql import JSONB
import datetime as dt

class ReportBase(SQLModel):
    target_url: str
    task_prompt: str

class Report(ReportBase, table=True):
    id: Optional[uuid.UUID] = Field(default_factory=uuid.uuid4, primary_key=True)
    status: str = Field(default="PENDING", index=True)
    # Use `Dict[str, Any]` for type-safe JSONB
    result: Optional[Dict[str, Any]] = Field(default=None, sa_column=Column(JSONB))
    created_at: dt.datetime = Field(default_factory=dt.datetime.utcnow, nullable=False)
    updated_at: dt.datetime = Field(default_factory=dt.datetime.utcnow, nullable=False, sa_column_kwargs={"onupdate": dt.datetime.utcnow})

class ReportCreate(ReportBase):
    pass # This will be the Pydantic model for API input

class ReportRead(ReportBase):
    id: uuid.UUID
    status: str
    created_at: dt.datetime-e
-e
-e
File: backend/src/api/v1/endpoints/__init__.py
-e
-e
-e
File: backend/src/api/v1/endpoints/agent.py
from fastapi import APIRouter, Depends, status
from sqlmodel.ext.asyncio.session import AsyncSession
from src.db.postgresql import get_db_session
from src.db.models.agent_run import AgentRunCreate, AgentRunRead
from src.services import agent_runner

# This would come from your security/auth logic
# from src.api.v1.auth import get_current_user

router = APIRouter()

# Placeholder for dependency injection of the current user
async def get_current_user_placeholder():
    # In a real app, this would decode a JWT and fetch the user from the DB
    return {"id": "mock_user_id_123"}

@router.post("/run", status_code=status.HTTP_202_ACCEPTED, response_model=AgentRunRead)
async def create_agent_run(
    run_in: AgentRunCreate,
    db: AsyncSession = Depends(get_db_session),
    current_user: dict = Depends(get_current_user_placeholder) # Secure this endpoint
):
    """
    Endpoint to create a new agent run.
    """
    run = await agent_runner.queue_agent_run(db=db, run_in=run_in, owner_id=current_user["id"])
    return run-e
-e
-e
File: backend/src/api/v1/__init__.py
from fastapi import APIRouter

from .endpoints import agent

router = APIRouter()

router.include_router(agent.router, prefix="/agent", tags=["Agent"])
-e
-e
-e
File: backend/src/api/v1/models/__init__.py
-e
-e
-e
File: backend/src/api/__init__.py
from .v1 import router-e
-e
-e
File: backend/src/worker/tasks.py
# backend/src/worker/tasks.py
import dramatiq
import asyncio
import redis.asyncio as redis
import json
import base64
from playwright.async_api import async_playwright, Page, Browser
import httpx

from .broker import redis_broker
from src.core.settings import get_settings

# This is the crucial import. We're bringing in the parser from the forge.
# This works because of your `setup.py` and editable install.
from forge.utils.function_parser import parse_function_call

settings = get_settings()
# In a real app, you would define this in your settings.py
INFERENCE_SERVER_URL = "http://inference:8001/predict"

# Create one Redis client for the module to reuse connections
redis_client = redis.Redis(host=settings.REDIS_HOST, port=settings.REDIS_PORT, auto_close_connection_pool=False)

async def execute_action(page: Page, action_str: str):
    """Parses and executes a single action string using Playwright."""

    # Use our robust parser to turn the string into a structured object
    parsed_calls = parse_function_call(action_str)
    if not parsed_calls:
        await redis_client.publish(f"logs:{run_id}", f"Parser Error: Could not parse action '{action_str}'")
        return

    # We assume the model only returns one action at a time
    call = parsed_calls[0]
    action_name = call.function_name
    params = call.parameters

    # Get the viewport size to convert normalized coordinates to pixels
    viewport_size = page.viewport_size
    if not viewport_size:
        # Fallback if viewport is not available for some reason
        viewport_size = {'width': 1920, 'height': 1080}

    await redis_client.publish(f"logs:{run_id}", f"Executing: {action_name} with params: {params}")

    try:
        if action_name == "click":
            if 'x' in params and 'y' in params:
                pixel_x = params['x'] * viewport_size['width']
                pixel_y = params['y'] * viewport_size['height']
                await page.mouse.click(pixel_x, pixel_y)
            else:
                raise ValueError("Click action requires 'x' and 'y' parameters.")

        elif action_name == "type":
            if 'text' in params:
                # Use keyboard for more realistic typing
                await page.keyboard.type(params['text'], delay=50) # Small delay for realism
            else:
                raise ValueError("Type action requires a 'text' parameter.")

        elif action_name == "scroll":
            if 'direction' in params and 'amount' in params:
                # Playwright scrolls in pixels, so we need a multiplier
                scroll_multiplier = 100
                delta_x = 0
                delta_y = 0
                if params['direction'] == 'down':
                    delta_y = params['amount'] * scroll_multiplier
                elif params['direction'] == 'up':
                    delta_y = -params['amount'] * scroll_multiplier

                await page.mouse.wheel(delta_x=delta_x, delta_y=delta_y)
            else:
                raise ValueError("Scroll action requires 'direction' and 'amount' parameters.")

        elif "TERMINATE" in action_name.upper():
            # This is handled in the main loop, so we just log it here.
            pass

        else:
            raise NotImplementedError(f"Action '{action_name}' is not implemented in the worker.")

    except Exception as e:
        error_message = f"Execution Error: Failed to execute {action_name}. Reason: {e}"
        print(error_message)
        await redis_client.publish(f"logs:{run_id}", error_message)


async def agent_task_async(run_id: str, target_url: str, task_prompt: str):
    """The async core of the agent run. Manages Playwright and the main loop."""
    print(f"🚀 [WORKER] Starting async task for run_id: {run_id}")

    async with async_playwright() as p:
        browser: Browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(viewport={"width": 1920, "height": 1080})
        page: Page = await context.new_page()

        await page.goto(target_url, wait_until="domcontentloaded", timeout=30000)
        await redis_client.publish(f"logs:{run_id}", f"Navigated to {target_url}")

        history: list[str] = []
        MAX_STEPS = 20 # Safety break

        for step in range(MAX_STEPS):
            await redis_client.publish(f"logs:{run_id}", f"--- Step {step + 1} ---")

            # 1. Capture Screenshot
            screenshot_bytes = await page.screenshot(type='jpeg', quality=70)
            base64_frame = base64.b64encode(screenshot_bytes).decode('utf-8')

            # 2. Publish Frame for Live View
            await redis_client.publish(f"frames:{run_id}", base64_frame)

            # 3. Get Next Action from Inference Server
            action_str = ""
            thought = ""
            async with httpx.AsyncClient(timeout=60.0) as client:
                try:
                    prompt = f"History: {history}\nTask: {task_prompt}"
                    response = await client.post(
                        INFERENCE_SERVER_URL,
                        json={"image_base64": base64_frame, "prompt": prompt}
                    )
                    response.raise_for_status()
                    result = response.json()
                    thought = result.get("thought", "No thought provided.")
                    action_str = result.get("action", "TERMINATE('No action provided.')")
                except Exception as e:
                    thought = f"CRITICAL ERROR: Could not get action from inference server. Reason: {e}"
                    action_str = f"TERMINATE('{thought}')"

            # 4. Publish Logs
            await redis_client.publish(f"logs:{run_id}", f"Thought: {thought}")

            # 5. Execute the Parsed Action
            await execute_action(page, action_str)
            history.append(action_str)

            # 6. Check for Termination
            if "TERMINATE" in action_str.upper():
                await redis_client.publish(f"logs:{run_id}", "Mission terminated by agent.")
                break

            await asyncio.sleep(2) # Wait for page to update after action

        await browser.close()

    final_result = {"status": "Completed", "log": history}
    # In a real app, you'd update the DB here with the final_result
    await redis_client.publish(f"logs:{run_id}", json.dumps(final_result))
    print(f"✅ [WORKER] Finished task for run_id: {run_id}")


@dramatiq.actor(broker=redis_broker, max_retries=1, time_limit=600_000) # 10 minute timeout
def run_churninator_agent(run_id: str, target_url: str, task_prompt: str):
    """
    This is the synchronous Dramatiq actor that wraps our async logic.
    """
    # Define a global variable for run_id to be accessible by the helper function
    global current_run_id
    current_run_id = run_id
    try:
        asyncio.run(agent_task_async(run_id, target_url, task_prompt))
    finally:
        asyncio.run(redis_client.close())

# This is a bit of a hack to get run_id into the execute_action function
# A cleaner way would be to use a class-based actor or partial functions.
current_run_id: str = ""-e
-e
-e
File: backend/src/worker/__init__.py
-e
-e
-e
File: backend/src/worker/broker.py
import dramatiq
from dramatiq.brokers.redis import RedisBroker
from src.core.settings import get_settings

# Configure the Redis broker
redis_broker = RedisBroker(host=get_settings().REDIS_HOST, port=get_settings().REDIS_PORT)
dramatiq.set_broker(redis_broker)-e
-e
-e
File: backend/src/main.py
# backend/src/main.py
from contextlib import asynccontextmanager
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from src.core.settings import get_settings
from src.db.postgresql import engine

# Ensure Dramatiq tasks are discovered by importing them
from src.worker import tasks
from src.api import router as api_router

@asynccontextmanager
async def lifespan(app: FastAPI):
    # On Startup
    print("🚀 Starting Churninator API...")
    yield
    # On Shutdown
    print("🔌 Shutting down Churninator API...")
    await engine.dispose()
    print("Database connection pool closed.")

settings = get_settings()
app = FastAPI(
    title=settings.PROJECT_NAME,
    debug=settings.DEBUG,
    lifespan=lifespan
)

# --- Middleware ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=[str(origin) for origin in settings.CORS_ORIGINS],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- API Routers ---
app.include_router(api_router, prefix=settings.API_V1_STR)

@app.get("/", tags=["Health"])
def read_root():
    return {"status": "ok", "project": settings.PROJECT_NAME}-e
-e
-e
File: backend/src/services/agent_runner.py
from sqlmodel.ext.asyncio.session import AsyncSession
from src.worker.tasks import run_churninator_agent
from src.db.models.agent_run import AgentRun, AgentRunCreate

async def queue_agent_run(db: AsyncSession, run_in: AgentRunCreate, owner_id: str) -> AgentRun:
    """Creates an AgentRun record and queues the background task."""

    # Create the database record, linking it to the owner
    db_run = AgentRun.model_validate(run_in, update={"owner_id": owner_id})
    db.add(db_run)
    await db.commit()
    await db.refresh(db_run)

    # Send the job to the Dramatiq worker
    run_churninator_agent.send(
        str(db_run.id),
        db_run.target_url,
        db_run.task_prompt
    )

    return db_run-e
-e
-e
File: web/next-env.d.ts
/// <reference types="next" />
/// <reference types="next/image-types/global" />
/// <reference path="./.next/types/routes.d.ts" />

// NOTE: This file should not be edited
// see https://nextjs.org/docs/app/api-reference/config/typescript for more information.
-e
-e
-e
File: web/next.config.ts
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;
-e
-e
-e
File: web/src/app/layout.tsx
import type { Metadata } from "next";
import { Geist, Geist_Mono } from "next/font/google";
import "./globals.css";

const geistSans = Geist({
  variable: "--font-geist-sans",
  subsets: ["latin"],
});

const geistMono = Geist_Mono({
  variable: "--font-geist-mono",
  subsets: ["latin"],
});

export const metadata: Metadata = {
  title: "Create Next App",
  description: "Generated by create next app",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased`}
      >
        {children}
      </body>
    </html>
  );
}
-e
-e
-e
File: web/src/app/page.tsx
import Image from "next/image";

export default function Home() {
  return (
    <div className="font-sans grid grid-rows-[20px_1fr_20px] items-center justify-items-center min-h-screen p-8 pb-20 gap-16 sm:p-20">
      <main className="flex flex-col gap-[32px] row-start-2 items-center sm:items-start">
        <Image
          className="dark:invert"
          src="/next.svg"
          alt="Next.js logo"
          width={180}
          height={38}
          priority
        />
        <ol className="font-mono list-inside list-decimal text-sm/6 text-center sm:text-left">
          <li className="mb-2 tracking-[-.01em]">
            Get started by editing{" "}
            <code className="bg-black/[.05] dark:bg-white/[.06] font-mono font-semibold px-1 py-0.5 rounded">
              src/app/page.tsx
            </code>
            .
          </li>
          <li className="tracking-[-.01em]">
            Save and see your changes instantly.
          </li>
        </ol>

        <div className="flex gap-4 items-center flex-col sm:flex-row">
          <a
            className="rounded-full border border-solid border-transparent transition-colors flex items-center justify-center bg-foreground text-background gap-2 hover:bg-[#383838] dark:hover:bg-[#ccc] font-medium text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5 sm:w-auto"
            href="https://vercel.com/new?utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app"
            target="_blank"
            rel="noopener noreferrer"
          >
            <Image
              className="dark:invert"
              src="/vercel.svg"
              alt="Vercel logomark"
              width={20}
              height={20}
            />
            Deploy now
          </a>
          <a
            className="rounded-full border border-solid border-black/[.08] dark:border-white/[.145] transition-colors flex items-center justify-center hover:bg-[#f2f2f2] dark:hover:bg-[#1a1a1a] hover:border-transparent font-medium text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5 w-full sm:w-auto md:w-[158px]"
            href="https://nextjs.org/docs?utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app"
            target="_blank"
            rel="noopener noreferrer"
          >
            Read our docs
          </a>
        </div>
      </main>
      <footer className="row-start-3 flex gap-[24px] flex-wrap items-center justify-center">
        <a
          className="flex items-center gap-2 hover:underline hover:underline-offset-4"
          href="https://nextjs.org/learn?utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app"
          target="_blank"
          rel="noopener noreferrer"
        >
          <Image
            aria-hidden
            src="/file.svg"
            alt="File icon"
            width={16}
            height={16}
          />
          Learn
        </a>
        <a
          className="flex items-center gap-2 hover:underline hover:underline-offset-4"
          href="https://vercel.com/templates?framework=next.js&utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app"
          target="_blank"
          rel="noopener noreferrer"
        >
          <Image
            aria-hidden
            src="/window.svg"
            alt="Window icon"
            width={16}
            height={16}
          />
          Examples
        </a>
        <a
          className="flex items-center gap-2 hover:underline hover:underline-offset-4"
          href="https://nextjs.org?utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app"
          target="_blank"
          rel="noopener noreferrer"
        >
          <Image
            aria-hidden
            src="/globe.svg"
            alt="Globe icon"
            width={16}
            height={16}
          />
          Go to nextjs.org →
        </a>
      </footer>
    </div>
  );
}
-e
-e
-e
File: ./.pre-commit-config.yaml
# ==============================================================================
# Global Hooks - Apply to all files in the repository
# ==============================================================================
repos:
-   repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.6.0
    hooks:
    -   id: check-added-large-files
        args: ['--maxkb=2048'] # No files > 2MB (models should be gitignored)
    -   id: check-case-conflict
    -   id: check-merge-conflict
    -   id: check-yaml
    -   id: end-of-file-fixer
    -   id: trailing-whitespace

-   repo: https://github.com/Yelp/detect-secrets
    rev: v1.5.0
    hooks:
    -   id: detect-secrets
        args: ['--baseline', '.secrets.baseline'] # Generate baseline first

# ==============================================================================
# Python Hooks - Apply to `backend/` and `forge/` directories
# ==============================================================================
-   repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.4.4
    hooks:
    # Run the linter
    -   id: ruff
        args: [--fix, --exit-non-zero-on-fix]
        files: ^(backend|forge)/
    # Run the formatter
    -   id: ruff-format
        files: ^(backend|forge)/

-   repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.10.0
    hooks:
    -   id: mypy
        files: ^(backend|forge)/
        args: [--ignore-missing-imports]
        # Mypy can be slow, so you might want to run it manually or in CI
        # To enable it, uncomment the line above.
        additional_dependencies: [
            "types-PyYAML",
            "types-requests",
            "pydantic"
        ]

# ==============================================================================
# Frontend Hooks - Apply to `web/` directory
# ==============================================================================
-   repo: https://github.com/pre-commit/mirrors-prettier
    rev: v3.1.0
    hooks:
    -   id: prettier
        files: ^web/.*\.(js|jsx|ts|tsx|css|scss|md|json)$
        args: ['--write'] # Auto-format files

-   repo: https://github.com/pre-commit/mirrors-eslint
    rev: ''
    hooks:
    -   id: eslint
        files: ^web/.*\.(js|jsx|ts|tsx)$
        args: [--fix]
        additional_dependencies:
        -   eslint@8.57.0
        -   typescript@5.4.5
        -   '@typescript-eslint/parser@7.9.0'
        -   '@typescript-eslint/eslint-plugin@7.9.0'
        -   'eslint-plugin-react@7.34.1'
        -   'eslint-config-next@14.2.3'

# ==============================================================================
# Frontend Type Checking - A special case
# ==============================================================================
-   repo: local
    hooks:
    -   id: tsc-no-emit
        name: Run tsc --noEmit
        entry: bash -c 'cd web && npm run typecheck || exit 1'
        language: system
        files: ^web/.*\.(ts|tsx)$
-e
-e
-e
File: ./backend/src/worker/entrypoint.sh
#!/bin/bash
set -e

# This entrypoint script is run from inside the Docker container.
# The project source code is located at /app/src.

echo "--- Churninator Worker Entrypoint ---"

# --- No `pip install -e .` needed ---
# Because we set PYTHONPATH=/app/src, Python can already find all our modules.
# We just need to install Playwright's browsers.

echo "[1/3] Installing Playwright browsers..."
playwright install --with-deps

echo "[2/3] Starting virtual display (Xvfb) on display :99..."
Xvfb :99 -screen 0 1920x1080x24 &
export DISPLAY=:99
sleep 2

# --- CORRECTED COMMAND ---
# We now call dramatiq and point it to the correct module path inside `src`.
echo "[3/3] Launching Dramatiq worker..."
exec uv run dramatiq -p 4 -t 4 src.worker.tasks-e
-e
-e
File: ./claude_context.sh
#!/bin/bash
#
# Churninator - Context Generation Script v2.0 (Final)
# Gathers all RELEVANT source, configs, and scripts, with robust exclusions.
#

echo "--- Generating complete context for The Churninator ---"

# --- Step 1: Clear previous context ---
echo "[1/5] Clearing old context file..."
> context.txt

# --- Step 2: Append Forge & Backend Source (Python) ---
echo "[2/5] Appending Forge & Backend source files (*.py)..."
find forge backend -type f -name "*.py" \
  -not -path "*/.venv/*" \
  -not -path "*/__pycache__/*" \
  -exec sh -c '
  echo "File: {}" >> context.txt && cat {} >> context.txt && echo -e "\n-e \n-e" >> context.txt
' \;

# --- Step 3: Append Frontend Source (Next.js) ---
echo "[3/5] Appending Frontend source files (*.ts, *.tsx)..."
find web -type f \( -name "*.ts" -o -name "*.tsx" \) \
  -not -path "*/node_modules/*" \
  -not -path "*/.next/*" \
  -exec sh -c '
  echo "File: $1" >> context.txt && cat "$1" >> context.txt && echo -e "\n-e \n-e" >> context.txt
' sh {} \;

# --- Step 4: Append Configs & Scripts (YAML & Shell) ---
echo "[4/5] Appending Configs (*.yaml) & Scripts (*.sh)..."
# THE FIX IS HERE: We now exclude .venv, .git, and node_modules from this global find.
find . -type f \( -name "*.yaml" -o -name "*.yml" -o -name "*.sh" \) \
  -not -path "./.git/*" \
  -not -path "*/.venv/*" \
  -not -path "*/node_modules/*" \
  -not -path "./forge/data/raw/*" \
  -exec sh -c '
  echo "File: {}" >> context.txt && cat {} >> context.txt && echo -e "\n-e \n-e" >> context.txt
' \;

# --- Step 5: Append Directory Trees & Final Prompt ---
echo "[5/5] Appending directory trees and project prompt..."
{
  echo "--- DIRECTORY TREES ---"
  echo ""
  echo "Forge Tree:"
  tree -I '.venv|__pycache__|data/raw|checkpoints|*.egg-info' forge
  echo ""
  echo "Backend Tree:"
  tree -I '.venv|__pycache__|.pytest_cache' backend
  echo ""
  echo "Frontend Tree (web/):"
  tree -I 'node_modules|.next|out' web
  echo ""
  echo "-----------------------"
  echo ""
} >> context.txt

# Append your startup context at the bottom
cat <<'EOT' >> context.txt
Project Context: The Churninator - Autonomous AI Mystery Shopper for SaaS

Core Concept: An open-source, AI-powered platform designed to autonomously analyze the signup and onboarding experience of any web application. It acts as a "mystery shopper," identifying UX friction points, conversion killers, and usability issues, then presents the findings in a detailed, interactive report.

System Architecture: Multi-Component, Cloud-Native Platform

1. The Forge (forge/):
Role: The "AI Workshop & ML Pipeline."
Function: Contains all scripts for the two-stage AGUVIS fine-tuning methodology (Stage 1: Grounding, Stage 2: Reasoning). It's designed for full-scale training on remote cloud GPUs (like Vast.ai).

2. The Backend (backend/):
Role: The "Mission Control & Production Engine."
Sub-components:
- API / Control Plane (backend/api/): FastAPI server for user requests and job queuing (Redis).
- Agent Worker (backend/worker/): Scalable Celery workers running Playwright in a virtual display (Xvfb).
- Inference Server (backend/inference/): Hosts the fine-tuned Churninator model (e.g., using vLLM), acting as the remote brain for the workers.

3. The Frontend (web/):
Role: The "Observatory & Executive Dashboard."
Function: A Next.js application with a "live view" dashboard (WebSockets for logs, MJPEG for video) to observe agents in real-time.

User Experience & Core Loop:
A user submits a URL and task. The API queues a job. A Worker picks it up, streams its browser view and "thoughts" to the frontend, and generates a final report on all discovered UX issues.
EOT

echo "--- Context generation complete. File 'context.txt' is ready. ---"
-e
-e
-e
File: ./forge/config.yaml
# ==============================================================================
# The Churninator Forge - Master Configuration
# ==============================================================================

# --- API KEYS & TRACKING ---
# Global credentials for services.
hf_token: "hf_YOUR_HUGGINGFACE_TOKEN_HERE"
wandb_project: "TheChurninatorForge"
wandb_entity: "your_wandb_username" # <-- CHANGE THIS

# --- CORE MODEL SELECTION ---
# The open-source VLM that will be forged into a Churninator.
base_model: "HuggingFaceTB/SmolLM-1.7B-32k-instruct"

# --- PATH CONFIGURATION ---
# Defines the physical layout of the Forge.
data_dir: "forge/data"
output_dir: "forge/checkpoints"

# --- DEFAULT HYPERPARAMETERS ---
# These are the default settings for all training runs.
# A run-specific config (e.g., stage1_grounding.yaml) can override any of these.
defaults:
  # Training Strategy
  training_mode: "lora" # qlora is enabled by the quantization_config in train.py
  num_train_epochs: 1
  learning_rate: 1.0e-4
  optim: "paged_adamw_8bit" # Efficient optimizer for QLoRA

  # Batching
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16 # Effective batch size = (batch_size * grad_accum)

  # Hardware & Precision
  bf16: true
  fp16: false
  gradient_checkpointing: true

  # LoRA (Low-Rank Adaptation) Defaults
  # These are good starting points for efficient fine-tuning.
  lora_r: 16          # Rank of the update matrices. Higher is more expressive but uses more memory.
  lora_alpha: 32      # LoRA scaling factor. Often 2 * lora_r.
  lora_dropout: 0.05  # Dropout probability for LoRA layers.

  # Logging
  report_to: "wandb"
  logging_steps: 10
  save_strategy: "epoch" # Options: "no", "epoch", "steps"
  save_steps: 500        # Only used if save_strategy is "steps"
  save_total_limit: 2    # Only keep the last 2 checkpoints to save disk space
-e
-e
-e
File: ./forge/train/config/stage1_grounding.yaml
# Blueprint for a Stage 1 "Grounding" Fine-Tune

# --- DATA & OUTPUT ---
data_file: "stage1_grounding.jsonl"
output_dir: "churninator-smollm-1.7b-grounded-v1"

# --- OVERRIDES ---
lora_r: 32
lora_alpha: 64
lora_dropout: 0.1

save_strategy: "steps"
save_steps: 2000
-e
-e
-e
File: ./forge/train/config/stage2_reasoning.yaml
# Blueprint for a Stage 2 "Reasoning" Fine-Tune

# --- MODEL, DATA & OUTPUT ---
# CRITICAL: We start from our Stage 1 model, not the base model.
base_model: "forge/checkpoints/churninator-smollm-1.7b-grounded-v1/final"
data_file: "stage2_reasoning.jsonl"
output_dir: "churninator-smollm-1.7b-reasoning-v1"

# --- OVERRIDES ---
learning_rate: 5.0e-5
per_device_train_batch_size: 1
gradient_accumulation_steps: 32
-e
-e
-e
File: ./forge/data/download_eval_data.sh
#!/bin/bash
# This script downloads the ScreenSpot benchmark dataset for evaluation.
# Run from the project root: bash forge/data/download_eval_data.sh

set -e

EVAL_DATA_DIR="forge/data/eval_data"
REPO="https://huggingface.co/datasets/HongxinLi/ScreenSpot_v2"

echo "🔥 Downloading ScreenSpot_v2 evaluation dataset..."
echo "Target directory: $EVAL_DATA_DIR"

mkdir -p "$EVAL_DATA_DIR"
cd "$EVAL_DATA_DIR"

if [ -d "ScreenSpot_v2" ]; then
    echo "ScreenSpot_v2 directory already exists. Skipping download."
else
    GIT_LFS_SKIP_SMUDGE=1 git clone "$REPO" ScreenSpot_v2
    cd ScreenSpot_v2
    git lfs pull
    cd ..
    echo "✅ ScreenSpot_v2 download complete."
fi

cd ../../.. # Return to project root
-e
-e
-e
File: ./forge/data/download_aguvis.sh
#!/bin/bash
# This script downloads the full, raw AGUVIS datasets from the Hugging Face Hub.
# It should be run ON THE REMOTE (Vast.ai) MACHINE.
# Requires git and git-lfs to be installed.

set -e

# --- Configuration ---
RAW_DATA_DIR="./raw" # Download into a 'raw' subdirectory
STAGE1_REPO="https://huggingface.co/datasets/xlangai/aguvis-stage1"
STAGE2_REPO="https://huggingface.co/datasets/xlangai/aguvis-stage2"

# --- Main Logic ---
echo "🔥 Starting FULL AGUVIS dataset download..."
echo "Target directory: $RAW_DATA_DIR"

mkdir -p "$RAW_DATA_DIR"
cd "$RAW_DATA_DIR"

echo "--------------------------------------------------"
echo "Downloading Stage 1: Grounding Dataset..."
if [ -d "aguvis-stage1" ]; then
    echo "Stage 1 directory already exists. Skipping."
else
    GIT_LFS_SKIP_SMUDGE=1 git clone "$STAGE1_REPO"
    cd aguvis-stage1 && git lfs pull && cd ..
    echo "✅ Stage 1 download complete."
fi

echo "--------------------------------------------------"
echo "Downloading Stage 2: Reasoning Dataset..."
if [ -d "aguvis-stage2" ]; then
    echo "Stage 2 directory already exists. Skipping."
else
    GIT_LFS_SKIP_SMUDGE=1 git clone "$STAGE2_REPO"
    cd aguvis-stage2 && git lfs pull && cd ..
    echo "✅ Stage 2 download complete."
fi

echo "✅ All raw materials are now on the remote machine."
-e
-e
-e
File: ./scripts/sync_from_vast.sh
#!/bin/bash

# --- Configuration ---
REMOTE_USER="root"
REMOTE_HOST="<IP_ADDRESS>"
REMOTE_PORT="<PORT>"

# The remote directory with your results
REMOTE_DIR="/workspace/forge/checkpoints/"
# The local destination
LOCAL_DIR="./forge/checkpoints/"

echo "🔽 Syncing results from the remote machine..."

# Create the local directory if it doesn't exist
mkdir -p "$LOCAL_DIR"

rsync -avz \
  -e "ssh -p $REMOTE_PORT" \
  "$REMOTE_USER@$REMOTE_HOST:$REMOTE_DIR" \
  "$LOCAL_DIR"

echo "✅ Results sync complete."
-e
-e
-e
File: ./scripts/sync_to_vast.sh
#!/bin/bash

# --- Configuration ---
REMOTE_USER="root"
REMOTE_HOST="<IP_ADDRESS>"
REMOTE_PORT="<PORT>"

# The remote directory with your results
REMOTE_DIR="/workspace/forge/checkpoints/"
# The local destination
LOCAL_DIR="./forge/checkpoints/"

echo "🔽 Syncing results from the remote machine..."

# Create the local directory if it doesn't exist
mkdir -p "$LOCAL_DIR"

rsync -avz \
  -e "ssh -p $REMOTE_PORT" \
  "$REMOTE_USER@$REMOTE_HOST:$REMOTE_DIR" \
  "$LOCAL_DIR"

echo "✅ Results sync complete."
-e
-e
-e
File: ./docker-compose.yml
-e
-e
-e
--- DIRECTORY TREES ---

Forge Tree:
forge
├── __init__.py
├── config.yaml
├── data
│   ├── __init__.py
│   ├── download_aguvis.sh
│   ├── download_eval_data.sh
│   ├── eval_data
│   │   └── ScreenSpot_v2
│   │       ├── data
│   │       │   ├── test-00000-of-00002.parquet
│   │       │   └── test-00001-of-00002.parquet
│   │       └── README.md
│   ├── preprocess.py
│   ├── processing
│   │   └── __init__.py
│   └── raw
│       ├── aguvis-stage1
│       │   ├── guienv.json
│       │   ├── guienvs.zip
│       │   ├── omniact_fix.json
│       │   ├── omniact.zip
│       │   ├── README.md
│       │   ├── ricoig16k.json
│       │   ├── ricoig16k.zip
│       │   ├── ricosca.json
│       │   ├── ricosca.zip
│       │   ├── seeclick_mi.json
│       │   ├── seeclick.json
│       │   ├── seeclick.tar.gz.part_00
│       │   ├── seeclick.tar.gz.part_01
│       │   ├── seeclick.tar.gz.part_02
│       │   ├── seeclick.tar.gz.part_03
│       │   ├── seeclick.tar.gz.part_04
│       │   ├── seeclick.tar.gz.part_05
│       │   ├── ui_refexp.json
│       │   ├── ui_refexp.zip
│       │   ├── webui350k.json
│       │   ├── webui350k.zip
│       │   ├── widget_captioning.json
│       │   └── widget_captioning.zip
│       └── aguvis-stage2
│           ├── aitw-l1.json
│           ├── aitw-l2.json
│           ├── aitw-l3.json
│           ├── aitw.zip
│           ├── amex-l1.json
│           ├── amex-l2.json
│           ├── amex-l3.json
│           ├── amex.zip
│           ├── android_control.json
│           ├── android_control.zip
│           ├── coat.json
│           ├── coat.zip
│           ├── gui-odyssey-l1.json
│           ├── gui-odyssey-l2.json
│           ├── gui-odyssey-l3.json
│           ├── gui-odyssey.tar.gz.part_00
│           ├── gui-odyssey.tar.gz.part_01
│           ├── gui-odyssey.tar.gz.part_02
│           ├── gui-odyssey.tar.gz.part_03
│           ├── guiact-web-multi-l1.json
│           ├── guiact-web-multi-l2.json
│           ├── guiact-web-multi-l3.json
│           ├── guiact-web-multi.zip
│           ├── guiact-web-single.json
│           ├── guiact-web-single.zip
│           ├── guide.json
│           ├── guide.zip
│           ├── mind2web-l1.json
│           ├── mind2web-l2.json
│           ├── mind2web-l3.json
│           ├── mind2web.zip
│           ├── miniwob-l1.json
│           ├── miniwob-l2.json
│           ├── miniwob-l3.json
│           ├── miniwob.zip
│           └── README.md
├── docker
│   ├── Dockerfile.inference
│   └── Dockerfile.train
├── eval
│   ├── __init__.py
│   ├── eval_dataset.py
│   ├── eval_prompt.py
│   └── eval.py
├── notebooks
├── pyproject.toml
├── README.md
├── train
│   ├── __init__.py
│   ├── collator.py
│   ├── config
│   │   ├── stage1_grounding.yaml
│   │   └── stage2_reasoning.yaml
│   ├── datasets.py
│   └── train.py
├── utils
│   ├── __init__.py
│   ├── action_conversion.py
│   └── function_parser.py
└── uv.lock

15 directories, 87 files

Backend Tree:
backend
├── Dockerfile
├── pyproject.toml
├── src
│   ├── __init__.py
│   ├── api
│   │   ├── __init__.py
│   │   └── v1
│   │       ├── __init__.py
│   │       ├── endpoints
│   │       │   ├── __init__.py
│   │       │   └── agent.py
│   │       └── models
│   │           └── __init__.py
│   ├── core
│   │   ├── __init__.py
│   │   ├── security.py
│   │   └── settings.py
│   ├── db
│   │   ├── __init__.py
│   │   ├── migrations
│   │   ├── models
│   │   │   ├── __init__.py
│   │   │   ├── agent_run.py
│   │   │   ├── report.py
│   │   │   └── user.py
│   │   └── postgresql.py
│   ├── main.py
│   ├── services
│   │   └── agent_runner.py
│   ├── utils
│   │   └── __init__.py
│   └── worker
│       ├── __init__.py
│       ├── broker.py
│       ├── Dockerfile
│       ├── entrypoint.sh
│       └── tasks.py
└── uv.lock

13 directories, 26 files

Frontend Tree (web/):
web
├── Dockerfile
├── eslint.config.mjs
├── next-env.d.ts
├── next.config.ts
├── package-lock.json
├── package.json
├── postcss.config.mjs
├── public
│   ├── file.svg
│   ├── globe.svg
│   ├── next.svg
│   ├── vercel.svg
│   └── window.svg
├── README.md
├── src
│   └── app
│       ├── favicon.ico
│       ├── globals.css
│       ├── layout.tsx
│       └── page.tsx
├── tsconfig.json
└── tsconfig.tsbuildinfo

4 directories, 19 files

-----------------------

Project Context: The Churninator - Autonomous AI Mystery Shopper for SaaS

Core Concept: An open-source, AI-powered platform designed to autonomously analyze the signup and onboarding experience of any web application. It acts as a "mystery shopper," identifying UX friction points, conversion killers, and usability issues, then presents the findings in a detailed, interactive report.

System Architecture: Multi-Component, Cloud-Native Platform

1. The Forge (forge/):
Role: The "AI Workshop & ML Pipeline."
Function: Contains all scripts for the two-stage AGUVIS fine-tuning methodology (Stage 1: Grounding, Stage 2: Reasoning). It's designed for full-scale training on remote cloud GPUs (like Vast.ai).

2. The Backend (backend/):
Role: The "Mission Control & Production Engine."
Sub-components:
- API / Control Plane (backend/api/): FastAPI server for user requests and job queuing (Redis).
- Agent Worker (backend/worker/): Scalable Celery workers running Playwright in a virtual display (Xvfb).
- Inference Server (backend/inference/): Hosts the fine-tuned Churninator model (e.g., using vLLM), acting as the remote brain for the workers.

3. The Frontend (web/):
Role: The "Observatory & Executive Dashboard."
Function: A Next.js application with a "live view" dashboard (WebSockets for logs, MJPEG for video) to observe agents in real-time.

User Experience & Core Loop:
A user submits a URL and task. The API queues a job. A Worker picks it up, streams its browser view and "thoughts" to the frontend, and generates a final report on all discovered UX issues.
