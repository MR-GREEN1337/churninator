# docker/Dockerfile.inference.mac (Corrected & Final)

FROM python:3.13-slim-bookworm

WORKDIR /app
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    # We create the venv here and add it to the path immediately
    VIRTUAL_ENV=/app/.venv \
    PATH="/app/.venv/bin:$PATH" \
    PYTHONPATH=/app

# --- Install UV and Create Environment ---
RUN pip install uv
RUN uv venv

# --- Install Dependencies ---
# We install dependencies first, without any source code present.
# This makes the build more cacheable and robust.
RUN uv pip install \
    "fastapi" \
    "uvicorn[standard]" \
    "pydantic" \
    "Pillow" \
    "transformers" \
    "torch" \
    "accelerate" \
    "peft" \
    --find-links https://download.pytorch.org/whl/cpu

# --- Copy and Install Project Code ---
COPY ./setup.py .
COPY ./forge/utils /app/forge/utils
COPY ./backend/src /app/backend/src

# Now that dependencies are installed, we install our own code.
RUN uv pip install -e .

# --- Final Configuration ---
RUN mkdir -p /app/models
EXPOSE 8001
CMD ["uvicorn", "backend.inference.server:app", "--host", "0.0.0.0", "--port", "8001"]
