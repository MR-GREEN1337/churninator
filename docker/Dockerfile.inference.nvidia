# docker/Dockerfile.inference.nvidia
FROM vllm/vllm-openai:latest

WORKDIR /app
ENV PYTHONUNBUFFERED=1 \
    PYTHONPATH=/app

RUN pip install uv

COPY backend/inference/pyproject.toml ./
RUN uv sync

COPY setup.py .
COPY backend/src ./backend/src
COPY forge/utils ./forge/utils

RUN uv pip install -e .

RUN mkdir -p /app/models
EXPOSE 8001
CMD ["uvicorn", "backend.inference.server:app", "--host", "0.0.0.0", "--port", "8001"]
