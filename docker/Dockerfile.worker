# backend/inference/Dockerfile.inference (Corrected & Final)

# ==============================================================================
# The Churninator - High-Performance Inference Server (vLLM Edition)
# ==============================================================================
# vLLM provides official Docker images with all CUDA dependencies pre-installed.
FROM vllm/vllm-openai:latest

# --- Stage 1: Setup & WORKDIR ---
WORKDIR /app
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# --- Stage 2: Install UV and Dependencies ---
RUN pip install uv

# Copy the specific dependency files from the monorepo root into the WORKDIR
COPY ./backend/inference/pyproject.toml /app/pyproject.toml
COPY ./uv.lock* /app/

# Use uv to install our app's dependencies.
RUN uv sync --locked --no-dev || uv sync --no-dev

# --- Stage 3: Copy Code & Perform Install ---
# Set the PYTHONPATH so the server can import from 'forge' and 'src'
ENV PYTHONPATH=/app

# Copy the setup file from the monorepo root
COPY ./setup.py /app/setup.py

# Copy all relevant source code directories needed for inference
COPY ./forge/utils /app/forge/utils
COPY ./backend/src /app/src

# Perform an "editable" install of our project. This makes modules like
# `forge.utils.function_parser` importable by the server.
RUN uv pip install -e .

# --- Stage 4: Final Configuration ---
# Create the directory for mounting models.
RUN mkdir -p /app/models

# Expose the API port.
EXPOSE 8001

# Run the inference server using uvicorn.
CMD ["uvicorn", "src.inference.server:app", "--host", "0.0.0.0", "--port", "8001"]
