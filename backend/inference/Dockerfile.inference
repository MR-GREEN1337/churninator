# ==============================================================================
# The Churninator - High-Performance Inference Server (vLLM Edition)
# ==============================================================================
# vLLM provides official Docker images with all CUDA dependencies pre-installed.
# This is the most reliable way to build.
FROM vllm/vllm-openai:latest

# --- Stage 1: System Dependencies & Setup ---
# The base image already contains Python, CUDA, and PyTorch.
# We just need to install our application dependencies.
WORKDIR /app
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# --- Stage 2: Install UV and Python Dependencies ---
# Install uv for fast dependency management.
RUN pip install uv

# Copy only the dependency files first for layer caching.
COPY ./pyproject.toml ./uv.lock* ./

# Use uv to install our app's dependencies.
# We don't need a venv as we are in a controlled container environment.
RUN uv pip install -r requirements.txt --system

# Copy and perform an editable install of our project code.
COPY ./setup.py .
RUN uv pip install -e . --system

# --- Stage 3: Copy Application Code ---
COPY ./backend/src /app/src

# --- Final Configuration ---
# Set the PYTHONPATH so Python can find our 'src' and 'forge' modules
ENV PYTHONPATH=/app

# Expose the API port.
EXPOSE 8001

# Run the inference server using uvicorn.
CMD ["uvicorn", "src.inference.server:app", "--host", "0.0.0.0", "--port", "8001"]
