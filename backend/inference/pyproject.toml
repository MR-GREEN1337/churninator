[project]
name = "inference"
version = "0.1.0"
description = "Churninator inference server"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "fastapi",
    "pydantic",
    "uvicorn[standard]",
    "python-dotenv",
    "torch",
    "transformers",
    "Pillow",
    "accelerate",
    "peft",
    "torchvision"
]

[project.optional-dependencies]
# Dependencies for running on NVIDIA GPUs in production
nvidia = [
    "vllm>=0.10.2",
    "torch"
]

# Dependencies for running on Apple Silicon (M-series) Mac
mac = [
    # vLLM does not support MPS. We will use the standard transformers library for local inference.
    "torch; sys_platform == 'darwin'",
    "accelerate"
]
